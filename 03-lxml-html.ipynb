{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7044a28d-13c4-4963-8838-c74fa360a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pdf2txt --layoutmode=normal --char-margin=10 -o xml/0.c.html PathofPurification2011.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5a4eb1f-354c-4644-899a-40c5218451c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "Number of elements per page\n",
      "(2, 11, 7, 7, 8, 18, 14, 7, 13, 9, 9, 9, 12, 15, 8, 12, 12, 9, 14, 10, 28, 22, 11, 16, 13, 18, 13, 14, 15, 41, 55, 45, 11, 15, 25, 11, 12, 11, 12, 14, 14, 11, 12, 13, 14, 14, 13, 12, 13, 16, 16, 11, 13, 13, 14, 14, 13, 5, 6, 5, 6, 5, 18, 16, 20, 16, 18, 20, 22, 18, 16, 38, 16, 13, 16, 18, 17, 12, 18, 15, 16, 15, 15, 16, 13, 14, 16, 14, 14, 15, 13, 14, 17, 20, 16, 18, 17, 18, 15, 16, 15, 20, 19, 19, 13, 18, 19, 13, 15, 20, 22, 16, 16, 16, 21, 18, 33, 20, 18, 20, 18, 20, 22, 23, 14, 20, 20, 20, 18, 27, 21, 18, 17, 18, 10, 5, 6, 5, 17, 15, 17, 17, 15, 17, 21, 16, 15, 14, 14, 13, 17, 20, 15, 16, 15, 17, 21, 24, 22, 14, 15, 21, 23, 15, 15, 15, 15, 16, 16, 11, 13, 20, 17, 20, 15, 17, 15, 14, 15, 15, 18, 17, 14, 13, 14, 16, 19, 17, 15, 13, 16, 12, 17, 15, 17, 18, 16, 14, 16, 15, 15, 15, 17, 12, 18, 16, 16, 16, 16, 18, 19, 19, 18, 15, 13, 15, 13, 21, 10, 17, 16, 18, 19, 19, 19, 13, 17, 19, 16, 16, 20, 19, 19, 17, 16, 14, 19, 22, 12, 15, 15, 19, 11, 15, 19, 23, 18, 17, 20, 32, 21, 14, 14, 15, 15, 18, 17, 18, 17, 14, 14, 19, 25, 17, 15, 15, 18, 17, 18, 20, 17, 17, 18, 16, 22, 18, 17, 16, 19, 19, 15, 11, 17, 20, 19, 17, 24, 20, 20, 23, 18, 14, 16, 17, 17, 20, 22, 16, 13, 16, 18, 19, 18, 15, 17, 17, 16, 20, 19, 14, 18, 17, 17, 15, 15, 18, 18, 17, 16, 19, 17, 16, 12, 13, 16, 15, 16, 15, 18, 15, 17, 15, 18, 15, 14, 18, 20, 18, 17, 13, 13, 14, 18, 17, 13, 13, 15, 12, 19, 15, 19, 17, 18, 19, 15, 20, 19, 15, 14, 15, 14, 15, 16, 17, 20, 17, 19, 19, 27, 17, 16, 18, 17, 14, 15, 16, 12, 12, 14, 19, 15, 14, 12, 17, 14, 16, 16, 18, 15, 17, 23, 15, 13, 11, 14, 13, 13, 18, 19, 17, 16, 18, 15, 19, 14, 18, 19, 16, 13, 13, 16, 14, 13, 15, 22, 18, 14, 16, 17, 16, 24, 18, 14, 18, 17, 17, 16, 14, 16, 17, 16, 15, 14, 13, 15, 15, 15, 13, 12, 16, 19, 15, 15, 14, 14, 17, 15, 14, 14, 15, 16, 13, 16, 15, 14, 15, 12, 15, 13, 15, 13, 15, 16, 17, 15, 15, 15, 18, 17, 16, 16, 16, 14, 15, 16, 15, 15, 16, 19, 14, 20, 16, 18, 16, 14, 5, 6, 5, 25, 14, 28, 15, 23, 14, 19, 16, 19, 17, 16, 13, 12, 17, 19, 14, 13, 13, 13, 16, 14, 20, 19, 16, 16, 16, 17, 19, 17, 17, 20, 19, 18, 23, 17, 22, 19, 21, 16, 20, 17, 13, 15, 15, 21, 21, 23, 20, 25, 21, 21, 15, 21, 18, 14, 22, 19, 14, 16, 19, 13, 31, 16, 19, 17, 19, 16, 15, 17, 18, 20, 18, 17, 16, 18, 15, 18, 18, 17, 16, 13, 16, 23, 22, 19, 23, 23, 21, 17, 14, 20, 31, 16, 20, 17, 20, 19, 16, 20, 17, 16, 12, 15, 19, 13, 14, 17, 16, 15, 22, 12, 15, 12, 13, 14, 15, 18, 13, 16, 16, 13, 14, 16, 13, 13, 14, 10, 15, 17, 17, 13, 22, 18, 16, 14, 15, 16, 17, 13, 17, 20, 19, 15, 18, 17, 17, 15, 15, 20, 19, 15, 19, 19, 19, 17, 22, 16, 22, 20, 20, 11, 17, 17, 14, 14, 16, 20, 23, 18, 19, 15, 13, 17, 14, 16, 13, 12, 17, 17, 14, 17, 16, 16, 17, 18, 14, 16, 15, 17, 19, 15, 18, 20, 16, 14, 15, 20, 17, 18, 12, 15, 23, 16, 16, 13, 13, 12, 12, 17, 19, 16, 20, 18, 18, 19, 13, 16, 18, 11, 14, 16, 18, 13, 19, 12, 11, 14, 14, 15, 16, 12, 16, 20, 18, 16, 17, 18, 17, 21, 18, 17, 18, 16, 14, 17, 22, 19, 17, 20, 14, 17, 13, 15, 19, 15, 16, 11, 17, 15, 13, 13, 17, 15, 19, 18, 18, 23, 22, 17, 17, 16, 17, 16, 18, 16, 21, 25, 15, 15, 21, 16, 15, 19, 23, 20, 19, 12, 20, 16, 17, 17, 13, 18, 15, 12, 21, 18, 20, 10, 18, 16, 26, 15, 19, 20, 13, 14, 16, 15, 15, 16, 17, 18, 10, 14, 11, 12, 5, 8, 5, 35, 38, 42, 37, 39, 48, 41, 38, 47, 41, 51, 30, 38, 41, 47, 39, 45, 42, 41, 50, 56, 33, 29, 47, 45, 40, 59, 56, 40, 53, 60, 40, 41, 46, 55, 39, 14, 23, 31, 59, 49, 40, 90, 38, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import copy\n",
    "import string\n",
    "import roman\n",
    "from lxml.html.clean import Cleaner\n",
    "\n",
    "\n",
    "cleaner = Cleaner(remove_unknown_tags=False, safe_attrs_only=False, style=False, remove_tags=['br'])\n",
    "open('xml/0.c1.html','w').write(cleaner.clean_html(open('xml/0.c.html').read()))\n",
    "\n",
    "tree=etree.parse('xml/0.c1.html',etree.XMLParser())\n",
    "root=tree.getroot()\n",
    "body=root[0]\n",
    "print(body.tag)\n",
    "\n",
    "# remove XHTML namespaces which we don't need\n",
    "for elem in root.getiterator():\n",
    "    if isinstance(elem, (etree._Comment,etree._ProcessingInstruction)): continue\n",
    "    # Remove a namespace URI in the element's name\n",
    "    elem.tag=etree.QName(elem).localname\n",
    "\n",
    "\n",
    "# map custom encoding of the URWPalladioPali to unicode\n",
    "# + change linebreaks to space\n",
    "c0='áÁòÒóÓþÞìÌúÚíÍ¿÷²ðåõºøý' +'\\n'\n",
    "c1='āĀṅṄṇṆṭṬīÌūŪṃṂḷēĕḍṣōṛśḥ' +'\\n'\n",
    "c01=str.maketrans(c0,c1)\n",
    "preserve='Ññéàù©§üöïäë}\"\\n'\n",
    "punctuation=' .-,;:!—–+*=“”\\'‘’…()[]?&/'# +'\\n'\n",
    "# š is rendered as ṝ, should be removed (is in index, last line 763: \"m. šformation (citta-saòkhára)\", should be just formation)\n",
    "# ` is (1) extra at the beginning of §82: `In the description of ...\n",
    "#      (2) 718: \"has ceased-reckoned as `been and gone’\" should be opening single quote (looks the same in PDF, anyway)\n",
    "typos='š`'\n",
    "valid=set(string.ascii_letters+string.digits+preserve+typos+punctuation+c1)\n",
    "for p in root.xpath('//span'):\n",
    "    if p.text is None: continue\n",
    "    p.text=p.text.translate(c01)\n",
    "    if set(p.text)-valid:\n",
    "        print(set(p.text)-valid,tree.getelementpath(p),p.text)\n",
    "for p in root.xpath('//div'):\n",
    "    if p.text=='\\n': p.text=''\n",
    "    \n",
    "patTop=re.compile(r'\\btop:([0-9]+)px;')\n",
    "def getTop(e):\n",
    "    m=patTop.search(e.attrib['style'])\n",
    "    return int(m.group(1))\n",
    "patLeft=re.compile(r'\\bleft:([0-9]+)px;')\n",
    "def getLeft(e):\n",
    "    m=patLeft.search(e.attrib['style'])\n",
    "    if m is None: return -1 # print(e.attrib['style'])\n",
    "    return int(m.group(1))\n",
    "\n",
    "# detect page beginnings based on anchors (and remove those)\n",
    "# selects parent div which contains <a name=\"...\">\n",
    "pageTops=[]\n",
    "for p in root.xpath('//div[a[@name]]'):\n",
    "    pageTops.append(getTop(p))\n",
    "    del p[0]\n",
    "    \n",
    "pageTops.append(pageTops[-1]*2)\n",
    "    \n",
    "pageTops=np.array(pageTops)\n",
    "\n",
    "\n",
    "pageElems=[list() for i in range(len(pageTops))]\n",
    "for e in root.xpath('//div[contains(@style,\"position:absolute\")] | //span[contains(@style,\"position:absolute\")]'):\n",
    "    if 0:\n",
    "        if e.tag in ('span','div') and len(e)==0 and (e.text is None or e.text=='') and ('black 1px solid' not in e.attrib['style']): continue   \n",
    "    t=getTop(e)\n",
    "    pg=np.searchsorted(pageTops,t)\n",
    "    pageElems[pg-1].append(e)\n",
    "# sort elements on each page by height\n",
    "for pg in range(len(pageElems)): pageElems[pg].sort(key=lambda e: (getTop(e),getLeft(e)))\n",
    "print('Number of elements per page')\n",
    "print(tuple(np.array([len(pageElems[i]) for i in range(len(pageElems))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3d3c5b-8db7-4b74-b14f-132da9be29ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3657623"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## new book document, with pages elements\n",
    "##\n",
    "\n",
    "# create a completely new tree now, putting all element inside <page id=\"..\">\n",
    "book=etree.Element(\"book\")\n",
    "for pgno in itertools.count(1):\n",
    "    if pgno<=5: continue\n",
    "    if pgno>=845: break # discard tables at the end\n",
    "    if pgno>=len(pageTops): break\n",
    "    pgtop=pageTops[pgno]\n",
    "    page=etree.Element(\"page\",id=str(pgno+1),top=str(pgtop))\n",
    "    for e in pageElems[pgno]:\n",
    "        e.attrib['y']=str(getTop(e)-pgtop)\n",
    "        e.attrib['x']=str(getLeft(e))\n",
    "        page.append(copy.deepcopy(e))\n",
    "    book.append(page)\n",
    "    \n",
    "open('xml/book.1.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))\n",
    "    \n",
    "# cleanup styles for everything\n",
    "for e in book.xpath('//span | //div'):\n",
    "    style=e.attrib.get('style',None)\n",
    "    if not style: continue\n",
    "    def _pair(s): return (_:=s.split(':'))[0].strip(),_[1].strip()\n",
    "    ss=[_pair(item) for item in style.split(';') if len(item)>0]\n",
    "    s2=dict(ss)\n",
    "    # things we definitely want to drop\n",
    "    for k in ('writing-mode','width','position','top','left','height'): s2.pop(k,None)\n",
    "    # sematic fonts (family attribute)\n",
    "    if ff:=s2.pop('font-family',None):\n",
    "        if ff in ('URWPalladioPali','URWPalladioITU','TimesNewRoman','Times_BPS'): pass\n",
    "        elif ff.startswith('TT'): pass # special fonts for tables\n",
    "        elif ff in ('URWPalladioPali-Italic','TimesNewRomanPS-ItalicMT','Times_BPS,Italic'): e.attrib['family']='italic'\n",
    "        elif ff in ('URWPalladioPali-Bold','Times_BPS,Bold'): e.attrib['family']='bold'\n",
    "        elif ff in ('URWPalladioPali-BoldItalic','Times_BPS-BoldItalic'): e.attrib['family']='bold-italic'\n",
    "        else: raise ValueError(f'Unhandled font-family {ff}')\n",
    "    # size attribute\n",
    "    if sz:=s2.pop('font-size',None):\n",
    "        assert sz.endswith('px')\n",
    "        if sz[:-2] not in ('8','9'): e.attrib['size']=sz[:-2]\n",
    "    # print(s2)\n",
    "    if len(s2): e.attrib['style']='; '.join([f'{k}:{v}' for k,v in s2.items()])\n",
    "    else: del e.attrib['style']\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if 1:        \n",
    "    # drop footers with accesstoinsight\n",
    "    if 1:\n",
    "        for e in book.xpath('page/div[contains(span[2],\"accesstoinsight\")]'):\n",
    "            e.getparent().remove(e)\n",
    "    \n",
    "    # drop page bottom lines\n",
    "    for e in book.findall('page/span[@style=\"border:gray 1px solid\"]'):\n",
    "        assert e.text is None or e.text==' ' or e.text=='\\n' or e.text=='|'\n",
    "        e.getparent().remove(e)\n",
    "\n",
    "    # all top-level divs (paragraphs) have this, drop it\n",
    "    for e in book.xpath('//div[@style=\"border:textbox 1px solid\"]'): del e.attrib['style']\n",
    "\n",
    "    # funny divs at the end of the page\n",
    "    for e in book.xpath('page/div[@x=\"-1\"]'): e.getparent().remove(e)\n",
    "\n",
    "    pgnosMain=[]\n",
    "    pgnosFront=[]\n",
    "    # remove page numbers (always last div on page)\n",
    "    for e in book.xpath('page/div[@y>=\"605\"]'):\n",
    "        pgno=int(e.getparent().attrib['id'])\n",
    "        if len(e)!=1: continue\n",
    "        if e[0].text is None: continue\n",
    "        # print(pgno,e[0].text)\n",
    "        #pgnos.append(e[0].text.strip())\n",
    "        t=e[0].text.strip()\n",
    "        if sum([c in t for c in 'xivcl'])>0: # front, roman numerals\n",
    "            import roman\n",
    "            pgnosFront.append(roman.fromRoman(t.replace(' ','').upper()))\n",
    "        else:\n",
    "            pgnosMain.append(int(t))\n",
    "        e.getparent().attrib['pageno']=e[0].text.strip()\n",
    "        e.getparent().remove(e)\n",
    "        \n",
    "    missingOk={'main':[78,79,80,428,429,430,748,749,750],'front':[]}\n",
    "    for pp,what in [(pgnosFront,'front'),(pgnosMain,'main')]:\n",
    "        # print(what,pp)\n",
    "        for i in range(pp[0],pp[-1]+1):\n",
    "            if i not in pp and i not in missingOk[what]:print(what,'missing footer',(roman.toRoman(i) if what=='front' else str(i)))\n",
    "\n",
    "    # remove page headings\n",
    "    for e in book.xpath('(page/div[1])[@y<=35]'): e.getparent().remove(e)\n",
    "    for e in book.xpath('(page/div[1])[@y<=35]'): e.getparent().remove(e)\n",
    "    \n",
    "    # drop these divs\n",
    "    for e in book.xpath('.//div[@style=\"border:figure 1px solid\"]'):\n",
    "        e.getparent().remove(e)\n",
    "\n",
    "# create <footnote_separator/> for horizontal lines at certain positions (heuristics)\n",
    "for e in book.xpath('//span[contains(@style,\"border:black 1px solid\") and (@x=\"40\" or @x=\"42\" or @x=\"48\" or @x=\"49\")]'):\n",
    "    if e.getparent().attrib['id']=='30' and int(e.attrib['y'])<200: continue\n",
    "    e.getparent().attrib['footnote_pos']=str(e.attrib['y'])\n",
    "    # print(e)\n",
    "    if 0: e.getparent().remove(e)\n",
    "    else:\n",
    "        e.tag='footnote_separator'\n",
    "        del e.attrib['style']\n",
    "\n",
    "## HACK\n",
    "for s in book.findall('.//span'):\n",
    "    if s.text!='107\\n': continue\n",
    "    pgno=s.getparent().getparent().attrib['id']\n",
    "    print(pgno)\n",
    "    s.getparent().remove(s)\n",
    "    \n",
    "# discard page with title after introduction\n",
    "for p in book.xpath('/book/page[@id=\"59\"]'): p.getparent().remove(p)\n",
    "    \n",
    "open('xml/book.2.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3490bd1c-1cb2-419d-ba5d-3a119338b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69: 8 → 8 (repeated footmark)\n",
      "pgno='499': error with size=5: e.text='' t0=' ', SKIPPING\n",
      "179: multiple matches for 11, using first match.\n",
      "@@\n",
      "@@\n",
      "@@\n",
      "30→29 30→29 30→29 30→29 30→29 30→29 30→29 30→29 36→35 36→35 36→35 36→35 36→35 36→35 36→35 36→35 36→35 36→35 36→35 36→35 36→35 36→35 56→55 56→55 56→55 56→55 56→55 56→55 56→55 56→55 56→55 56→55 56→55 56→55 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 81→80 87→86 88→87 88→87 88→87 99→98 133→132 133→132 133→132 133→132 133→132 133→132 133→132 133→132 133→132 133→132 133→132 165→164 165→164 165→164 178→177 178→177 178→177 178→177 178→177 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 180→179 193→192 193→192 193→192 193→192 193→192 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 245→244 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 252→251 253→251 253→251 253→251 253→251 253→251 253→251 255→254 255→254 255→254 255→254 257→256 257→256 257→256 257→256 257→256 257→256 257→256 257→256 257→256 258→257 258→257 258→257 258→257 258→257 258→257 258→257 259→258 259→258 259→258 264→263 264→263 264→263 264→263 264→263 265→264 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 267→266 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 292→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 293→291 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 324→323 341→340 341→340 341→340 342→341 342→341 342→341 343→342 343→342 343→342 343→342 343→342 343→342 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 346→345 377→376 377→376 377→376 377→376 377→376 377→376 388→387 388→387 388→387 388→387 388→387 388→387 388→387 388→387 417→416 417→416 417→416 420→419 420→419 423→422 452→451 452→451 452→451 452→451 452→451 452→451 481→480 481→480 481→480 481→480 481→480 481→480 490→489 490→489 490→489 490→489 490→489 490→489 490→489 490→489 490→489 490→489 490→489 490→489 490→489 492→491 492→491 492→491 492→491 492→491 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 494→493 499→498 499→498 499→498 502→501 502→501 502→501 502→501 502→501 502→501 502→501 502→501 504→503 504→503 504→503 504→503 504→503 505→504 506→505 506→505 506→505 506→505 506→505 506→505 506→505 506→505 506→505 506→505 506→505 506→505 506→505 507→506 507→506 507→506 507→506 507→506 507→506 507→506 507→506 507→506 507→506 507→506 508→506 512→511 512→511 512→511 512→511 512→511 512→511 512→511 512→511 513→512 515→514 515→514 515→514 515→514 515→514 515→514 515→514 526→525 541→540 547→546 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 548→547 552→551 552→551 552→551 552→551 552→551 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 582→581 583→581 583→581 583→581 583→581 583→581 583→581 583→581 583→581 583→581 583→581 583→581 585→584 585→584 585→584 585→584 585→584 585→584 585→584 585→584 585→584 585→584 585→584 585→584 597→596 597→596 597→596 597→596 597→596 597→596 597→596 597→596 597→596 632→631 632→631 632→631 632→631 632→631 665→664 665→664 665→664 665→664 665→664 665→664 665→664 665→664 665→664 665→664 665→664 665→664 665→664 672→671 672→671 681→680 681→680 681→680 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 691→690 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 714→713 715→714 726→725 726→725 726→725 726→725 726→725 726→725 726→725 744→743 744→743 746→745 746→745 746→745 766→765 766→765 769→768 \n",
      "----------------------------------------\n",
      "*29: 1 *33: 2 *34: 3,4 *35: 5,6,7,8 *37: 9 *39: 10 *40: 11 *41: 12 *42: 13 *44: 14,15 *45: 16 *46: 17 *48: 18 *49: 19 *50: 20 *51: 21,22 *53: 23 *54: 24 [23] *55: 25,26 *56:  [26] *63: 1 *64: 2,3 *65: 4 *66: 5 *67: 6 *68: 7 *69: 8 *73: 9 *76: 10 *77: 11 *79: 12 *80: 13,14 *81: 15 *82: 16 *83: 17,18 *84: 19,20 *85: 21,22 *86: 23,24 *87: 25 *88: 26 *89: 27,28 *91: 29 *96: 30 *97: 31 *98: 32 *99: 33,34 *102: 35,36 *103: 37 *106: 38,39,40 *111: 41 *114: 1,2,3 *115: 4 *116: 5,6 *117: 7,8 *118: 9 *119: 10 *120: 11,12 *123: 13 *124: 14 *128: 15 *129: 16 *130: 17 *132: 18,20,19 *133: 21 [20] *139: 1,2 *140: 3,4,5,6 *142: 7 *145: 8,9,11,10,12 *146: 13 *147: 14 *148: 15 *151: 16 *152: 17 *155: 18,19 *156: 20 *157: 21 *158: 22 *159: 23 *160: 24,25 *161: 26 *162: 27 *163: 28 *164: 29,30 *165: 31 *166: 32,33,34,35 *167: 36,37 *168:  [37] *172: 1 *173: 2,3 *175: 4 *176: 5 *177: 6,7,8 *178: 9,10  (11,12) *179: 11,12,13 *180:  [11,12] *181: 14,15,16 *187: 17 *189: 18 *190: 19,20,21 *191: 22 *192: 23,24 *194: 25,26 *195: 27,28 *196: 29 *197: 30,31 *199: 32 *200: 33 *201: 34,35,36 *202: 37 *203: 38,39 *205: 40 *206: 41,42 *207: 43 *209: 44 *210: 45 *211: 46,47 *213: 48 *215: 49 *216: 50 *220: 1,2 *222: 3,4 *224: 5 *226: 6,7 *227: 1,2 *230: 3 *231: 4 *232: 5,6 *233: 7,8 *234: 9 *235: 10 *237: 11 *238: 12 *241: 13 *244: 1 *246: 2,3 *248: 4  (5) *249: 5 *250:  [5]  (6) *251: 6,7 *252:  [6] *253: 8 *254: 9,10 *255: 11 *256: 12,13 *257: 14 *258: 15  (16) *259: 16,17,18 *260: 19,20 [16] *262: 21,22,23 *263: 24 [23] *264: 25 *265: 26,27 *266: 28,29 *268: 30 *269: 31,32 *271: 33,34,35 *272:  [35] *275: 36 *283: 1,2 *284: 3 *285: 4 *286: 5,6,7 *288: 8 *289: 9 *290: 10 *291: 11 *293: 12 *294: 13 *296: 14 *297: 15,16 *300: 17 *301: 18 *302: 19,20,21 *303: 22 *305: 23 *306: 24,25 *307: 26,27 *308: 28 *309: 29,30 *310: 31,32,33 *312: 34  (35) *313: 35 *314:  [35] *316: 36,37 *318: 38 *319: 39,40,41 *321: 42,43  (44) *322: 44,45 *323: 46,47 [44] *325: 48 *326: 49,50 *328: 51 *329: 52 *331: 53,54 *333: 55,56 *334: 57,58 *335: 59 *336: 60,61 *337: 62 *338: 63 *340: 64,65 *341: 66 *342: 67,68 *345: 69 *346: 70,71,72 *347: 73,74 *350: 1 *351: 2 *352: 3 *353: 4 *358: 5 *362: 6 *363: 7,8 *365: 9 *367: 10 *369: 11 *370: 12,13,14 *371: 15 *373: 16 *374: 17,18 *375: 19 [18] *376: 20,21 *377:  [21] *378: 22 *380: 1 *381: 2,3,4 *382: 5,6 *384: 7,8 *387: 9 *389: 10 *391: 11 *395: 1,2 *396: 3,4 *397: 5,6,7,8,9,10 *398: 11,12,13 [8] *399: 14 *400: 15,16 *401: 17,18 *402: 19 *403: 20 *404: 21,22 *405: 23,24 *406: 25 *408: 26 *411: 27 *414: 28 *416: 29 *417: 30,31,32,33 *418: 34,35,36,37 *419: 38 *421: 39 *422: 40 *423: 41 *424: 42,43 *425: 44,45 *426: 46 *427: 1 *429: 2 *430: 3 *431: 4,5 *435: 6 *437: 7 *438: 8,9 *440: 10 *441: 11 *442: 12,13 *443: 14,15,16 *446: 17 *450: 18,19 *451: 20 *455: 21 *456: 22 *459: 1,2,3,4 *461: 5,6 *462: 7 *463: 8,9 *464: 10 *467: 11,12 *468: 13,14 *470: 15 *471: 16 *474: 17 *477: 18,19 *480: 20,21,22 *481: 23 *482: 24,25 *489: 1  (2) *491: 2,3,4 *492: 5 [2] *493: 6 *494: 7 *495: 8  (9) *496: 9,10,11 *497: 12 [9] *498: 13,14 *499: 15,16  (17) *500: 17,18,19,20 *501: 21,22 [17] *503: 23,24 *504: 25 *505: 26 *506: 27 *508: 28,29 *510: 30,31 *511: 32 *512: 33 *513: 34 *514: 35,36 *517: 37 *518: 38,39,40 *519: 41 *520: 42,43 *521: 44,45,46 *522: 47,48 *523: 49,50,51,52,53 *524: 54 *525: 55,56 *526: 57  (58) *527: 58,59 *528: 60,61,62 [58] *529: 63,64 *531: 65 *532: 66 *533: 67 *535: 68 *536: 69 *537: 70 *539: 71 *540: 72,73,74 *541: 75,76 *542: 77 *545: 78 *546: 79,80,81 *547: 82,83 *548: 84  (1) *551: 1,3,2 *552: 4 [1,3] *553: 5,6,7 *554: 8,9 *555: 10,11 *556: 12,13 *557: 14 *558: 15 *559: 16,17,18 *560: 19,20 *561: 1 *562: 2 *563: 3 ****************************************************************************************************F 564: 4\n",
      " (4) *565: 4,5,6 *566:  [4] *568: 7 *569: 8,9,10 *570: 11 *574: 12,13 *576: 14 *577: 15 *578: 16,17 *581: 18 *583: 19,20,21,22 *584: 23 *585: 24 *587: 25 *588: 26 *591: 1 *595: 2 *596: 3,4 *597: 5,6 *603: 7 *604: 8,9 *608: 10 *609: 11,12 *610: 13,14 *611: 15 *616: 16 *617: 17 *618: 18 *619: 19 *623: 20,21 *625: 22 *626: 23 *627: 24 *629: 25 *630: 26,27 *631: 28,29,30 *632: 31 *633: 32 *635: 33 *636: 34 *637: 35 *638: 36 *639: 37,38 *647: 39 *648: 40,41 *650: 42 *657: 43 *660: 44 *662: 45 *663: 46 *664: 47,48 *667: 1,2,3 *668: 4,5 *669: 6,7 *671: 8 *674: 9 *680: 1 *682: 2 *683: 3,4 *684: 5,6 *689: 1 *690: 2,3 *695: 4,5 *696: 6,7,8,9,10,11 *697: 12,13,14 *698: 15 *700: 16 *701: 17 *702: 18,19 *703: 20 *706: 21,22 *709: 23 *710: 24,25 *711: 26 *713: 27,28 *714: 29 *715: 30 *718: 31,32 *719: 33,34 *720: 35 *721: 36 *722: 37 *723: 38 *724: 1,2 *725: 3 *726: 4 *727: 5,6,7 *728: 8 *729: 9 *730: 10,11,12 *735: 13 *736: 14,15 *737: 16 *739: 17,18,19 *740: 20,21,22,23 *741: 24,25,26,27,28 *742: 29,30 *743: 31,32,33,34 *744:  [34] *745: 35 *748: 36,37 *750: 38 *753: 39 *754: 40 *755: 41 *756: 42 *757: 43 *758: 44 *760: 1,2 *761: 3 *762: 4,5 *763: 6,7 *765: 8 *766: 9 *767: 10,11 *768: 12 *769: 13 *771: 14 *772: 15 *774: 16 *775: 17 *776: 18,19 *777: 20 *785: 21 *786: 22 *789: 1 *790: 2,3 *791: 4,5,6,7 *793: 8,9 *795: 10,11 *796: 12,13 *797: 14,15 *798: 16,17 *800: 18 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3681753"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## FOOTNOTES\n",
    "##\n",
    "\n",
    "from lxml import etree\n",
    "import itertools,copy,re,string\n",
    "\n",
    "book=tree=etree.parse('xml/book.2.xml',etree.XMLParser()).getroot() \n",
    "# footnotes \n",
    "\n",
    "\n",
    "# detect footnote marks (small font)\n",
    "footCurr=0\n",
    "for page in book:\n",
    "    pgno=page.attrib['id']\n",
    "    for e in page.findall('*/span[@size=\"5\"]'):\n",
    "        if e.text in (None,'st','nd','rd','th'): continue\n",
    "        t0=e.text[:]\n",
    "        hadEol=(e.text.endswith('\\n') or e.text.endswith('|'))\n",
    "        #if pgno=='285':\n",
    "        #    print(e.text,e.text.strip(),hadEol)\n",
    "        e.text=e.text.strip().replace('|','')\n",
    "        if set(e.text)-set(string.digits): raise RuntimeError(f'Unhandled size=5 text: \"{e.text}\"')\n",
    "        try: f=int(e.text)\n",
    "        except ValueError:\n",
    "            # raise RuntimeError(f'{pgno=}: {e.text=} {t0=}')\n",
    "            print(f'{pgno=}: error with size=5: {e.text=} {t0=}, SKIPPING')\n",
    "            continue\n",
    "        if f not in (footCurr+1,1): print(f'{pgno}: {footCurr} → {f} (repeated footmark)')\n",
    "        # print(f'{pgno} {f}')\n",
    "        footCurr=f\n",
    "        e.tag='footref'\n",
    "        e.attrib['page_id']=e.getparent().getparent().attrib['id']\n",
    "        del e.attrib['size']\n",
    "        # preserve linebreak after footnote (important in verse)\n",
    "        if hadEol:\n",
    "            eol=etree.Element('span')\n",
    "            eol.text='\\n'\n",
    "            e.getparent().insert(e.getparent().index(e)+1,eol)\n",
    "\n",
    "\n",
    "        \n",
    "# find footnote marks in footnote area\n",
    "\n",
    "pgDebug=-1\n",
    "footLast=0\n",
    "footVerbose=False\n",
    "# footrefs=[]\n",
    "for page in book:\n",
    "    global footLast\n",
    "    pgno=int(page.attrib['id'])\n",
    "    # print(f'{pgno} {footLast}')\n",
    "    # if pgno>=100: break\n",
    "    fy=int(page.attrib.get('footnote_pos','0'))\n",
    "    footrefs=[int(e.text) for e in page.findall('.//footref')]\n",
    "    if footrefs and footrefs[0]==1:\n",
    "        footLast=0\n",
    "    if fy==0: continue # no footnotes on this page\n",
    "    pageFootNums=[]\n",
    "    \n",
    "    if footrefs: footMin,footMax=footLast+1,footrefs[-1]+1\n",
    "    else: footMin,footMax=footLast+1,footLast+2\n",
    "    ## exceptions\n",
    "    if pgno==65: footMax=4\n",
    "    elif pgno==224: footMax=5\n",
    "    elif pgno==260: footMax=20\n",
    "    elif pgno==690: footMax=3\n",
    "    elif pgno==498: footMax=14\n",
    "    elif pgno==558: footMax=15\n",
    "    \n",
    "    if pgno==pgDebug or footVerbose: print(f'{pgno=} {footMin=} {footMax=} {footLast=} {footrefs=}')\n",
    "\n",
    "    divs=[d for d in page.findall(f'div') if int(d.attrib['y'])>fy]\n",
    "    for num in range(footMin,footMax+1):\n",
    "        numLone=re.compile(rf'^({num})$')\n",
    "        numPat=re.compile(rf'\\b({num})\\.')\n",
    "        numPatLine=re.compile(rf'\\n({num}).(?=\\n)')\n",
    "        numPatNoDot=re.compile(rf'\\b({num})\\b(?!\\))')\n",
    "        for currPat,strategy in (numLone,'$'),(numPatLine,'|'),(numPat,'='),(numPatNoDot,'#'):\n",
    "            if pgno==76 and strategy=='#': continue\n",
    "            dobreak=False\n",
    "            for div in divs:\n",
    "                for span in div:\n",
    "                    if span.tag!='span' or not span.text: continue\n",
    "                    et=span.text.replace(\"\\n\",\"|\")\n",
    "                    if pgno==pgDebug: print(f'{num=} {et=} {strategy=} {currPat=}')\n",
    "                    matches=list(currPat.finditer(span.text))\n",
    "                    if not matches: continue\n",
    "                    if len(matches)>1: print(f'{pgno}: multiple matches for {num}, using first match.')\n",
    "                    m=matches[0]\n",
    "                    if num!=footLast+1: print(f'{pgno}: non-sequential footnotes {footLast} → {num}')\n",
    "                    if span.text[:m.span()[0]].endswith(' note '):\n",
    "                        print('@@')\n",
    "                        continue\n",
    "                    footLast=num\n",
    "                    # replace elements\n",
    "                    m0,m1=m.span()\n",
    "                    if strategy=='|':\n",
    "                        if pageFootNums: # (pgno,num) in [(35,7),]:\n",
    "                            eLeft,eMiddle,eRight=etree.Element(\"span\"),etree.Element('footmark',id=m.group(1)),etree.Element(\"span\")\n",
    "                            eLeft.text=span.text[:m0]\n",
    "                            eRight.text=span.text[m1:]\n",
    "                            span.addprevious(eLeft)\n",
    "                            span.addprevious(eMiddle)\n",
    "                            span.addprevious(eRight)\n",
    "                            span.getparent().remove(span)\n",
    "                        else:\n",
    "                            span.text=span.text[:m0]+span.text[m1:]\n",
    "                            span.getparent().insert(0,etree.Element(\"footmark\",id=m.group(1)))\n",
    "                            print(f'[{pgno} {m.group(1)}]',end=\" \")\n",
    "                    else:\n",
    "                        if m0>0:\n",
    "                            e=etree.Element(\"span\")\n",
    "                            e.text=span.text[:m0]\n",
    "                            span.addprevious(e)\n",
    "                        span.addprevious(emark:=etree.Element(\"footmark\",id=m.group(1)))\n",
    "                        if m1<len(span.text):\n",
    "                            e=etree.Element(\"span\")\n",
    "                            e.text=span.text[m1:]\n",
    "                            span.addprevious(e)\n",
    "                        span.getparent().remove(span)\n",
    "                    pageFootNums.append(num)\n",
    "                    dobreak=True\n",
    "                    break\n",
    "                if dobreak: break\n",
    "            if dobreak: break\n",
    "open('xml/book.3.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))\n",
    "\n",
    "\n",
    "book=tree=etree.parse('xml/book.3.xml',etree.XMLParser()).getroot() # remove_blank_text=True)).getroot()\n",
    "# move overflown footnotes to the pages where they belong\n",
    "prevFooter=None\n",
    "for page in book:\n",
    "    pgno=int(page.attrib['id'])\n",
    "    sep=page.find('footnote_separator')\n",
    "    if sep is None:\n",
    "        prevFooter=None\n",
    "        # print(f'{pgno}: reset prevFooter')\n",
    "        continue\n",
    "    overflow=True\n",
    "    for footer in sep.itersiblings():\n",
    "        for e in footer:\n",
    "            if e.tag=='footmark': overflow=False\n",
    "            if overflow:\n",
    "                # hack\n",
    "                if pgno==581:\n",
    "                    footer.insert(0,etree.Element('footmark',id='18'))\n",
    "                    overflow=False\n",
    "                else:\n",
    "                    #print(f'({pgno})',end=\" \")\n",
    "                    print(f'{pgno}→{prevFooter.getparent().attrib[\"id\"]}',end=\" \")\n",
    "                    prevFooter.append(e)\n",
    "    if not overflow:\n",
    "        prevFooter=footer\n",
    "        #print(f'{pgno}: {prevFooter=}')\n",
    "    else:\n",
    "        pass\n",
    "        #print(f'{pgno}: still overflowing, {prevFooter=}')\n",
    "            \n",
    "#if 0:   \n",
    "#    p65=book.find('page[@id=\"65\"]')\n",
    "#    assert 'border:black 1px solid' in p65[-1].attrib['style']\n",
    "#    p65.remove(p65[-1])\n",
    "\n",
    "print('\\n'+20*'--')\n",
    "# second pass: put footnotes together\n",
    "pgDebug=564\n",
    "footVerb=True\n",
    "prevNotes={}\n",
    "prevRefs={}\n",
    "for page in book:\n",
    "    pgno=int(page.attrib['id'])\n",
    "    sep=page.find('footnote_separator')\n",
    "    # if sep is None and not prevNotes: continue\n",
    "    ff={}\n",
    "    # print('\\n** pg',pgno)\n",
    "    mark=None\n",
    "    if sep is not None:\n",
    "        # iterate through divs (\"paragraphs\") in the footer area\n",
    "        nextPara=False\n",
    "        for footer in sep.itersiblings():\n",
    "            marks=[]\n",
    "            if nextPara: ff[mark].append(etree.Element('div')) # new paragraph of the last footnote\n",
    "            for e in footer:\n",
    "                # this is a new footnote\n",
    "                if e.tag=='footmark':\n",
    "                    mark=e.attrib['id']\n",
    "                    marks.append(e)\n",
    "                    nextPara=False\n",
    "                    continue\n",
    "                if mark is None: raise RuntimeError(f'{pgno}')\n",
    "                if mark not in ff: ff[mark]=[etree.Element('div')]\n",
    "                ff[mark][-1].append(e)\n",
    "                # print(f'{pgno}: {mark}')\n",
    "            if mark and len(ff[mark][-1])>0: nextPara=True\n",
    "            for e in marks: e.getparent().remove(e)\n",
    "    for mark,elem in prevRefs.items():\n",
    "        for e in ff[mark]: elem.append(e)\n",
    "    if ff or prevNotes:\n",
    "        # print(f'{ff} {prevNotes}')\n",
    "        print(f'*{pgno}: {\",\".join(ff.keys())}',end=\" \")\n",
    "        if prevNotes: print(f'[{\",\".join(prevNotes.keys())}]',end=\" \")\n",
    "    prevRefs={}\n",
    "    # print()\n",
    "    #for footer in sep.itersiblings():\n",
    "    for ref in page.findall('.//footref'):\n",
    "        if pgno==pgDebug: print(100*'*'+f'F {pgno}: {ref.text}')\n",
    "        ref.tag='footnote'\n",
    "        ref.attrib['page_id']=page.attrib['id']\n",
    "        ref.attrib['mark']=ref.text\n",
    "        src=(prevNotes if ref.text in prevNotes else ff)\n",
    "        if pgno==69 and ref.text=='8' and '8' not in src:\n",
    "            ref.attrib['reference_existing_footnote']=\"1\"\n",
    "            # ref.append(etree.Element('footnote_extra_ref_TODO',id='8',page_id='69'))\n",
    "        elif pgno==397 and ref.text=='8': pass\n",
    "        elif ref.text in src:\n",
    "            for e in src.pop(ref.text): ref.append(e)\n",
    "        else:\n",
    "            # print(f'[{ref.text} unavailable]',list(prevNotes.keys()),list(ff.keys()))\n",
    "            prevRefs[ref.text]=ref\n",
    "        ref.text=None\n",
    "        if len(ref)==0 or len(ref[-1])==0: pass #  print('@')\n",
    "        else:\n",
    "            tail=ref[-1][-1].text.strip()\n",
    "            if tail.endswith('with Ce of M-a and A-a'): ref[-1][-1].text,tail=ref[-1][-1].text[:-1]+'.',tail+'.'\n",
    "            if sum([tail.endswith(s) for s in ['.','.”',')”','.]','?','.’',')','…”','”…','nābhinandati …',]])==0:\n",
    "                # print(f'\\n{pgno=} {ref.text} {tail[-30:]}')\n",
    "                #ref[-1][-1].text=ref[-1][-1].text+'(END-FOOTNOTE-ERROR)'\n",
    "                raise RuntimeError(f'Error in footnote ending {pgno=} {tail[:-30]}')\n",
    "    prevNotes=ff\n",
    "    # print(f'{pgno}: {list(ff.keys())} {list(prevNotes.keys())} {list(prevRefs.keys())}')\n",
    "    if prevRefs: print(f' ({\",\".join(prevRefs.keys())})',end=\" \")\n",
    "    \n",
    "# remove all separators        \n",
    "for page in book:\n",
    "    sep=page.find('footnote_separator')\n",
    "    if sep is None: continue\n",
    "    for f in sep.itersiblings():\n",
    "        # print(f'{page.attrib[\"id\"]} {f.tag} {len(f)=} {f.text}')\n",
    "        if f.tag=='div' and len(f)==0 and f.text=='\\n': f.getparent().remove(f)\n",
    "    #sep.getparent().remove(sep)\n",
    "    #del page.attrib['footnote_pos']\n",
    "\n",
    "open('xml/book.4.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fcf92f09-7a3d-40f7-be0d-57271433ccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 span 31 span 32 span "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3635190"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## paragraphs accross pages, small caps, verse\n",
    "##\n",
    "\n",
    "book=tree=etree.parse('xml/book.4.xml',etree.XMLParser()).getroot()\n",
    "# book=tree=etree.parse('vism.book.4.xml',etree.XMLParser()).getroot()\n",
    "\n",
    "# this is page 65\n",
    "for s in book.xpath('.//span[@style=\"border:black 1px solid\" and @y>500]'):\n",
    "    s.getparent().remove(s)\n",
    "\n",
    "\n",
    "# remove a few more useless tags\n",
    "for page in book:\n",
    "    pgno=int(page.attrib['id'])\n",
    "    for e in page.findall('.//div'):\n",
    "        if (\n",
    "            e.text in (None,'\\n','','|',' ') and ('style' not in e.attrib)\n",
    "            and (len(e)==0 or (len(e)==1 and e[0].tag=='span' and e[0].text in (None,'\\n','','|',' ')))\n",
    "            ): e.getparent().remove(e)\n",
    "    if len(page)==0: continue\n",
    "    if page[-1].tag=='footnote_separator':\n",
    "        page.remove(page[-1])\n",
    "    \n",
    "# try to detect Small Caps runs\n",
    "for page in book:\n",
    "    pgno=int(page.attrib['id'])\n",
    "    # if pgno>100: break\n",
    "    # if pgno!=145: continue\n",
    "    for span in page.findall('.//span'):\n",
    "        def _textIsNum(text):\n",
    "            return re.match('^(?P<head>\\([xivcm]+\\)) (?P<tail>[^a-z]+)$',text)\n",
    "        def _spanSc(s,ini=False):\n",
    "            if s.tag!='span' or s.text is None: return False\n",
    "            # if not ini: print(f'  test: {s.text}')\n",
    "            if sum(map(str.islower,s.text)) and not _textIsNum(s.text): return False\n",
    "            if (upper:=sum(map(str.isupper,s.text)))<(2 if ini else 0): return False\n",
    "            if ini and upper<len(s.text)//2: return False\n",
    "            if '.......' in s.text: return False # in TOC\n",
    "            return True\n",
    "        def _elemScCompat(s,s2,sizes):\n",
    "            if not _spanSc(s2,ini=False): return False\n",
    "            if not _textIsNum(s2.text) and s.attrib.get('family','normal')!=s2.attrib.get('family','normal'): return False\n",
    "            if len(sizes)==2 and s.attrib.get('size','9') not in sizes: return False\n",
    "            sizes.add(s.attrib.get('size','9'))\n",
    "            sizes.add(s2.attrib.get('size','9'))\n",
    "            return True\n",
    "        if not _spanSc(span,ini=True): continue\n",
    "        #  print(f'{pgno}: starting from \"{span.text}\"')\n",
    "        run=[span]\n",
    "        sizes=set()\n",
    "        for e in span.itersiblings(preceding=True):\n",
    "            if not _elemScCompat(span,e,sizes):\n",
    "                # print('no')\n",
    "                break\n",
    "            #print(f'  - \"{e.text}\"')\n",
    "            run.insert(0,e)\n",
    "        for e in span.itersiblings():\n",
    "            if not _elemScCompat(span,e,sizes): break\n",
    "            #print(f'  + \"{e.text}\"')\n",
    "            run.append(e)\n",
    "        if len(run)<2: continue\n",
    "        bigger=max([int(s) for s in sizes])\n",
    "        parent=span.getparent()\n",
    "        \n",
    "        if m:=_textIsNum(run[0].text):\n",
    "            parent.insert(parent.index(span),e:=etree.Element('span',size=str(bigger)))\n",
    "            e.text=m.group('head')\n",
    "            run[0].text=m.group('tail')            \n",
    "        span.text=''.join([s.text if int(s.attrib.get('size','9'))==bigger else s.text.lower() for s in run])\n",
    "        span.attrib['family']='smallcaps'\n",
    "        span.attrib['size']=str(bigger)\n",
    "        for e in run:\n",
    "            if e!=span: parent.remove(e)\n",
    "        #print(pgno,sizes,'_'.join([e.text for e in run]))\n",
    "        #print(pgno,span.text)\n",
    "\n",
    "        \n",
    "open('xml/book.4a.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))\n",
    "        \n",
    "# detect paragraphs continuing from preceding page\n",
    "prevDone=False\n",
    "for page in book:\n",
    "    pgno=int(page.attrib['id'])\n",
    "    if len(page)==0: continue\n",
    "    # if page[0].tag!='div': print(f'{pgno}')\n",
    "    d0=page[0]\n",
    "    if d0.tag!='div':\n",
    "        print(f'{pgno} {d0.tag}',end=\" \")\n",
    "        continue\n",
    "    assert d0.tag=='div'\n",
    "    if int(d0.attrib['y'])<60 and int(d0.attrib['x'])<50:\n",
    "        # unindented start of new paragraph, not continuation\n",
    "        if re.match('^[0-9]+\\. ',d0[0].text): continue\n",
    "        if prevDone: print(f'{pgno} ?')\n",
    "        d0.attrib['continuation']=\"1\"\n",
    "    prevDone=(page[-1][-1].text is not None and page[-1][-1].text.endswith('.'))\n",
    "    \n",
    "\n",
    "msg=False\n",
    "# remove table from introduction\n",
    "for div in list(book.xpath('page[(@id>=30) and (@id<=32)]/*[self::div or self::span]')):\n",
    "    pg,y=int(div.getparent().attrib['id']),int(div.attrib['y'])\n",
    "    # print(f'{div.tag=} {pg=} {y=}')\n",
    "    #if not (30<=pg<=32): continue\n",
    "    if pg==30 and y<150: continue\n",
    "    if pg==32 and y>350: continue\n",
    "    if not msg:\n",
    "        d2=etree.Element('div',**div.attrib)\n",
    "        del d2.attrib['style']\n",
    "        d2.append(etree.Element('TODO',id='ceylon-king-tab',desc='table with kings of Ceylon',y=div.attrib['y'],x=div.attrib['y']))\n",
    "        div.addprevious(d2)\n",
    "    msg=True\n",
    "    #print(pg,end=' ')\n",
    "    div.getparent().remove(div)\n",
    "\n",
    "\n",
    "    \n",
    "# detect verse divs\n",
    "for page in book:\n",
    "    # break\n",
    "    pgno=int(page.attrib['id'])\n",
    "    if pgno in (591,724): continue # hack: chapter title would be interpreted as verse\n",
    "    # if pgno!=273: continue\n",
    "    # print(pgno)\n",
    "    for div in page:\n",
    "        if not (112<int(div.attrib['x'])<125): continue\n",
    "        if sum([s.text.count('\\n') for s in div if s.text])<2: continue\n",
    "        div.tag='verse'\n",
    "        lines=[]\n",
    "        buf=[]\n",
    "        for s in div:\n",
    "            if s.text is None or '\\n' not in s.text:\n",
    "                buf.append(s)\n",
    "                continue\n",
    "            #print('2',s.text)\n",
    "            splits=s.text.split('\\n')\n",
    "            for i,l in enumerate(splits):\n",
    "                # print(f'{i} {l} {buf=}')\n",
    "                if len(l)>0:\n",
    "                    e=etree.Element('span')\n",
    "                    e.text=l\n",
    "                    for a in ('size','family'):\n",
    "                        if a in s.attrib: e.attrib[a]=s.attrib[a]\n",
    "                    buf.append(e)\n",
    "                if i<len(splits)-1:\n",
    "                    lines.append(buf)\n",
    "                    buf=[]\n",
    "        if buf: lines.append(buf)\n",
    "        for e in div: div.remove(e)\n",
    "        for line in lines:\n",
    "            el=etree.Element('line')\n",
    "            for e in line: el.append(e)\n",
    "            div.append(el)\n",
    "\n",
    "    \n",
    "    \n",
    "open('xml/book.4b.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c58bae16-36a2-4dcc-8f2c-76675b9fd9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 25 Message from his Holiness the Dalai Lama\n",
      "* 26 Publisher’s Foreword to Third Edition\n",
      "* 26 Publisher’s Foreword to Fourth Edition\n",
      "* 27 Translator’s Preface\n",
      "* 29 Introduction\n",
      "  * 29 Background and Main Facts\n",
      "  * 34 The|Visuddhimagga|and its Author\n",
      "  * 46 The|Vimuttimagga\n",
      "  * 47 Trends in the Development of Theravāda Doctrine\n",
      "  * 49 The|Paramatthamañjusā\n",
      "  * 51 Concerning the Translation\n",
      "  * 55 Concluding  Remarks\n",
      "            63 [I. Introductory]\n",
      "            68 [II. Virtue]\n",
      "68 5 : (i) |What is virtue?| It is the states beginning with volition present in one who\n",
      "abstains from kill\n",
      "            68 (i) What is virtue?\n",
      "?? 69 34 : as to the rest—|19. (ii) I|n what sense is it virtue?| It is virtue (|sīla|)| |in the sense of compo\n",
      "## 69 3 : 20. (iii) Now,  |what  are  its  characteristic,  function,  manifestation,  and  proximate|cause?| \n",
      "## 70 3 : 23. (iv) |What  are  the  benefits  of  virtue?|  Its  benefits  are  the  acquisition  of  the|seve\n",
      "## 72 2 : 25. (v) Now,  here  is  the  answer  to  the  question,  |How  many  kinds  of  virtue  are|there?|\n",
      "?? 106 4 : 143. However, it was also asked (vi) |What  is the defiling  of  it?| and |What  is  the|cleansing o\n",
      "            113 [The 13 kinds of Ascetic Practices]\n",
      "139 5 : (i) |What  is  concentration?|  Concentration  is  of  many  sorts  and  has  various\n",
      "aspects.  An  \n",
      "            139 (i) What  is  concentration?\n",
      "## 140 13 : 3. (ii) |In what sense is it concentration?| It is concentration (|samādhi|) in the sense|of concent\n",
      "## 140 5 : 4. (iii) |What are its characteristic, function, manifestation, and proximate cause?||Concentration \n",
      "## 140 2 : 5. (iv) |How many kinds of concentration are there?|\n",
      "            144 [A. Development in Brief]\n",
      "            145 [B. Development in Detail]\n",
      "            145 [The Ten Impediments]\n",
      "            171 [The Eighteen Faults  of  a Monastery]\n",
      "            174 [The Five Factors of  the Resting Place]\n",
      "            174 [The  Lesser  Impediments]\n",
      "            175 [Detailed  Instructions  for  Development]\n",
      "            175 [The Earth Kasiṇa]\n",
      "            176 [Making an Earth Kasiṇa]\n",
      "            177 [Starting  Contemplation]\n",
      "            178 [The Counterpart Sign]\n",
      "            179 [The Two Kinds of Concentration]\n",
      "            180 [Guarding the Sign]\n",
      "            182 [The Ten Kinds of Skill in Absorption]\n",
      "            188 [The  Five  Similes]\n",
      "            189 [Absorption  in  the Cognitive Series]\n",
      "            191 [The First Jhāna]\n",
      "            203 [Extension of the Sign]\n",
      "            206 [The Second Jhāna]\n",
      "            209 [The Third Jhāna]\n",
      "            214 [The Fourth Jhāna]\n",
      "            218 [The Fivefold Reckoning of Jhāna]\n",
      "            220 [The Water Kasiṇa]\n",
      "            221 [The Fire Kasiṇa]\n",
      "            221 [The Air Kasiṇa]\n",
      "            222 [The Blue Kasiṇa]\n",
      "            222 [The Yellow Kasiṇa]\n",
      "            223 [The Red Kasiṇa]\n",
      "            223 [The White Kasiṇa]\n",
      "            223 [The Light Kasiṇa]\n",
      "            224 [The Limited-Space Kasiṇa]\n",
      "            224 [General]\n",
      "            227 [General  Definitions]\n",
      "            228 [The  Bloated]\n",
      "            237 [The Livid]\n",
      "            237 [The  Festering]\n",
      "            237 [The Cut Up]\n",
      "            238 [The  Gnawed]\n",
      "            238 [The  Scattered]\n",
      "            238 [The Hacked  and  Scattered]\n",
      "            238 [The  Bleeding]\n",
      "            238 [The  Worm-Infested]\n",
      "            238 [A  Skeleton]\n",
      "            240 [General]\n",
      "            246 [(1) Recollection  of  the Enlightened One]\n",
      "                246 [Accomplished]\n",
      "                250 [Fully  Enlightened]\n",
      "                251 [Endowed With Clear Vision and Virtuous Conduct]\n",
      "                254 [Sublime]\n",
      "                256 [Knower of Worlds]\n",
      "                260 [Incomparable Leader  of Men  to  be Tamed]\n",
      "                261 [Teacher of Gods and Men]\n",
      "                262 [Enlightened]\n",
      "                262 [Blessed]\n",
      "            267 [(2) Recollection  of the Dhamma]\n",
      "                268 [Well  Proclaimed]\n",
      "                270 [Visible Here  and  Now]\n",
      "                271 [Not  Delayed]\n",
      "                271 [Inviting of Inspection]\n",
      "                272 [Onward-Leading]\n",
      "                272 [Is  Directly  Experienceable  by  the  Wise]\n",
      "            273 [(3) Recollection of the Saṅgha]\n",
      "                273 [Entered on the Good, Straight, True, Proper Way]\n",
      "                274 [Fit for Gifts]\n",
      "                275 [Fit  for  Hospitality]\n",
      "                275 [Fit for Offering]\n",
      "                275 [Fit for Salutation]\n",
      "                275 [As an Incomparable Field of Merit for the World]\n",
      "            276 [(4) Recollection  of Virtue]\n",
      "            278 [(5)  Recollection  of  Generosity]\n",
      "            279 [(6)  Recollection  of  Deities]\n",
      "                280 [General]\n",
      "            283 [(7) Mindfulness  of Death]\n",
      "            294 [(8) Mindfulness Occupied with the Body]\n",
      "            317 [(9) Mindfulness  of Breathing]\n",
      "            344 [(10)  Recollection  of  Peace]\n",
      "            349 [(1) Loving-Kindness]\n",
      "            366 [(2) Compassion]\n",
      "            367 [(3)  Gladness]\n",
      "            368 [(4)  Equanimity]\n",
      "            379 [(1) The Base Consisting of Boundless Space]\n",
      "?? 391 3 : [|General|]|\n",
      "            395 [Perception  of  Repulsiveness  in  Nutriment]\n",
      "            402 [Defining of The Elements: Word Definitions]\n",
      "            403 [Texts and Commentary  in Brief]\n",
      "            404 [In Detail]\n",
      "            406 [Method  of Development  in Brief]\n",
      "            407 [Method  of  Development  in  Detail]\n",
      "                407 [(1) With Constituents in Brief]\n",
      "                407 [(2)  With  Constituents  by  Analysis]\n",
      "                415 [(3) With Characteristics in Brief]\n",
      "                416 [(4) With Characteristics  by Analysis]\n",
      "            416 [Additional Ways  of Giving Attention]\n",
      "            425 [Development  of  Concentration—Conclusion]\n",
      "            425 [The Benefits  of Developing Concentration]\n",
      "            427 [The Benefits of Concentration (Continued)]\n",
      "                427 [(1) The Kinds of Supernormal Power]\n",
      "                458 [(2) The Divine Ear Element]\n",
      "                460 [(3) Penetration  of Minds]\n",
      "                462 [(4) Recollection  of Past Lives]\n",
      "                473 [(5) The Divine Eye—Knowledge of Passing Away and\n",
      "Reappearance  of Beings]\n",
      "                479 [General]\n",
      "            489 [A. Understanding]\n",
      "489 5 : (i) |What is understanding?| Understanding (|paññā|) is of many sorts and has various\n",
      "aspects.  An  \n",
      "            489 (i) What is understanding?\n",
      "## 489 9 : 3. (ii) |In what sense is it understanding?| It is understanding (|paññā|) in the sense|of  act  of \n",
      "## 491 5 : 7. (iii) |What  are  its  characteristic,  function,  manifestation  and  proximate  cause?||Underst\n",
      "## 491 2 : 8. (iv)  |How  many  kinds  of  understanding  are  there?|\n",
      "## 497 5 : 32. (v)  |How  is  it  developed?|  Now,  t|he  things  class|ed  as  aggregates,  bases,|elements, \n",
      "            497 [B. Description  of  the Five Aggregates]\n",
      "                497 [The  Materiality  Aggregate]\n",
      "                513 [The  Consciousness  Aggregate]\n",
      "            514 [The 89 Kinds of Consciousness—see Table III]\n",
      "            520 [The 14 Modes of Occurrence of Consciousness]\n",
      "                524 [The  Feeling  Aggregate]\n",
      "                526 [The  Perception  Aggregate]\n",
      "            526 [The Formations Aggregate—see Tables II & IV]\n",
      "            527 [According to Association with Consciousness]\n",
      "            539 [C. Classification  of  the Aggregates]\n",
      "            539 [Materiality]\n",
      "            542 [Feeling]\n",
      "                544 [Perception, Formations and Consciousness]\n",
      "            544 [D. Classes  of Knowledge  of  the Aggregates]\n",
      "            550 [A. Description of the Bases]\n",
      "            554 [B. Description  of  the Elements]\n",
      "            561 [A. Description of the Faculties]\n",
      "            564 [B. Description of the Truths]\n",
      "                568 [(i) Birth]\n",
      "                572 [(ii) Ageing]\n",
      "                572 [(iii) Death]\n",
      "                573 [(iv) Sorrow]\n",
      "                573 [(v)  Lamentation]\n",
      "                574 [(vi) Pain]\n",
      "                574 [(vii) Grief]\n",
      "                574 [(viii) Despair]\n",
      "                575 [(ix) Association with the Unloved]\n",
      "                575 [(x) Separation from the Loved]\n",
      "                575 [(xi) Not to Get What One Wants]\n",
      "                576 [(xii) The  Five Aggregates]\n",
      "            576 [The Truth of the Origin of Suffering]\n",
      "            577 [The Truth of the Cessation of Suffering]\n",
      "            578 [Discussion on Nibbāna]\n",
      "            582 [The Truth of the Way]\n",
      "            584 [General]\n",
      "            591 [Section A. Definition of Dependent Origination]\n",
      "            597 [Section B. Exposition]\n",
      "                597 [I.  Preamble]\n",
      "                598 [II. Brief Exposition]\n",
      "                605 [III. Detailed Exposition]\n",
      "                605 [(i) Ignorance]\n",
      "                606 [(ii) Formations]\n",
      "                637 [(iv)  Mentality-Materiality]\n",
      "                641 [(v) The Sixfold Base]\n",
      "                644 [(vi) Contact]\n",
      "                646 [(vii) Feeling]\n",
      "                647 [(viii) Craving]\n",
      "                648 [(ix) Clinging]\n",
      "                651 [(x) Becoming]\n",
      "?? 655 3 : [(xi)–(xii) Birth, Etc|.|]|\n",
      "            656 [Section C. The Wheel of Becoming]\n",
      "                656 [(i) The Wheel]\n",
      "                658 [(ii) The Three Times]\n",
      "                658 [(iii) Cause and Fruit]\n",
      "                661 [(iv) Various]\n",
      "            667 [Defining  of  Mentality-Materiality]\n",
      "                667 [(1) Definition Based on the Four Primaries]\n",
      "                670 [(2) Definition Based  on  the Eighteen Elements]\n",
      "                670 [(3) Definition Based  on  the Twelve Bases]\n",
      "                671 [(4) Definition Based  on  the Five Aggregates]\n",
      "                671 [(5) Brief Definition Based on the Four Primaries]\n",
      "            672 [If  the Immaterial Fails  to Become Evident]\n",
      "            672 [How  the  Immaterial  States  Become  Evident]\n",
      "            674 [No  Being  Apart  from  Mentality-Materiality]\n",
      "            676 [Interdependence  of  Mentality  and  Materiality]\n",
      "            679 [Ways of Discerning Cause and Condition]\n",
      "            679 [Neither Created  by  a  Creator  nor Causeless]\n",
      "            680 [Its Occurance is Always Due to Conditions]\n",
      "            680 [General  and Particular Conditions]\n",
      "            681 [Dependent Origination  in Reverse Order]\n",
      "            681 [Dependent Origination in Direct Order]\n",
      "            681 [Kamma  and  Kamma-Result]\n",
      "            685 [No Doer Apart  from Kamma  and Result]\n",
      "            686 [Full-Understanding of the Known]\n",
      "            689 [The Three Kinds of Full-Understanding]\n",
      "            691 [Insight: Comprehension by Groups]\n",
      "            693 [Comprehension by Groups—Application of Text]\n",
      "            695 [Strengthening of Comprehension in Forty Ways]\n",
      "            697 [Nine Ways of Sharpening the Faculties, Etc.\n",
      "            697 [Comprehension  of  the Material]\n",
      "                698 [(a)  Kamma-Born  Materiality]\n",
      "                699 [(b)  Consciousness-Born  Materiality]\n",
      "                700 [(c)  Nutriment-Born  Materiality]\n",
      "                701 [(d)  Temperature-Born  Materiality]\n",
      "            702 [Comprehension  of  the Immaterial]\n",
      "            703 [The  Material  Septad]\n",
      "            710 [The  Immaterial  Septad]\n",
      "            712 [The Eighteen Principal Insights]\n",
      "            715 [Knowledge of Rise and Fall—I]\n",
      "            718 [The Ten Imperfections of Insight]\n",
      "            725 [Insight: The Eight Knowledges]\n",
      "                725 [1. Knowledge of Rise and Fall—II]\n",
      "                726 [2. Knowledge  of Dissolution]\n",
      "                731 [3. Knowledge of Appearance as Terror]\n",
      "                733 [4. Knowledge of Danger]\n",
      "                736 [5. Knowledge of Dispassion]\n",
      "                737 [6. Knowledge  of Desire  for Deliverance]\n",
      "                737 [7. Knowledge  of Reflection]\n",
      "            739 [Discerning Formations as Void]\n",
      "                742 [8. Knowledge  of Equanimity  about Formations]\n",
      "            743 [The  Triple  Gateway  to  Liberation]\n",
      "            746 [The Seven Kinds  of Noble Persons]\n",
      "            747 [The Last Three Knowledges  are One]\n",
      "            748 [Insight Leading  to Emergence]\n",
      "            750 [The  Twelve  Similes]\n",
      "            753 [The Difference in the Noble Path’s Factors, Etc.]\n",
      "                756 [9. Conformity Knowledge]\n",
      "            757 [Sutta  References]\n",
      "            759 [I. Change-of-Lineage, Paths, and Fruits]\n",
      "            759 [The  First  Path—First  Noble  Person]\n",
      "            762 [The First Fruition—Second Noble Person]\n",
      "            763 [The Second Path—Third Noble Person]\n",
      "            764 [The Second Fruition—Fourth Noble Person]\n",
      "            764 [The Third Path—Fifth Noble Person]\n",
      "            764 [The Third Fruition—Sixth Noble Person]\n",
      "            764 [The  Fourth  Path—Seventh  Noble  Person]\n",
      "            765 [The Fourth Fruition—Eighth Noble Person]\n",
      "            765 [II. The States Associated with the Path, Etc.]\n",
      "            779 [The Four Functions]\n",
      "            779 [The Four Functions in a Single Moment]\n",
      "            781 [The Four Functions Described Separately]\n",
      "            786 [Conclusion]\n",
      "## 788 5 : 1. (vi)  W|hat  are  the  benefits  in  developing  understanding?|  (See  XIV.1)|  |[698]|\n",
      "            804 [Postscript]\n",
      "?? 805 1 : End|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3633254"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## SECTIONING\n",
    "## \n",
    "import numpy as np\n",
    "\n",
    "book=tree=etree.parse('xml/book.4b.xml',etree.XMLParser()).getroot()\n",
    "# detect headings\n",
    "for sp in book.xpath('.//div[starts-with(span,\"Part \")]'):\n",
    "    if len(sp)!=3 or sp[0].tag!='span' or not 'size' in sp[0].attrib or sp[0].attrib['size']!='26' or not sp[0].text.startswith('Part'): continue\n",
    "    # h=etree.Element('heading-1-part')\n",
    "    sp.tag='heading-1-part'\n",
    "    sp.attrib['toc_name']=sp[0].text.split('\\n')[0]\n",
    "    sp.attrib['toc_num']=sp.attrib['toc_name'].split()[-1]\n",
    "    ee=[]\n",
    "    for i,s in enumerate(sp):\n",
    "        e=etree.Element('span')\n",
    "        e.text=(s.text.split('\\n')[1] if i==0 else s.text).replace('\\n','')\n",
    "        if 'family' in s.attrib: e.attrib['family']=s.attrib['family']\n",
    "        ee.append(e)\n",
    "    for s in sp: sp.remove(s)\n",
    "    for e in ee: sp.append(e)\n",
    "\n",
    "# keep track of page numbers for chapters -- used for sections/subections below\n",
    "chap2page=[]\n",
    "for d1 in book.xpath('.//div[starts-with(span,\"Chapter \")]'):\n",
    "    # skip stuff in TOC\n",
    "    pgno=int(d1.getparent().attrib['id'])\n",
    "    if pgno<29: continue\n",
    "    chap2page.append(pgno)\n",
    "    d2=d1.getnext()\n",
    "    d3=d2.getnext()\n",
    "    d1.tag='heading-2-chapter'\n",
    "    d1.attrib['toc_name']=d1[0].text.strip()\n",
    "    d1.attrib['toc_num']=d1[0].text.strip().split()[-1]\n",
    "    d2.attrib['anchor']=d1.attrib['toc_num']\n",
    "    for c in d1: d1.remove(c)\n",
    "    def mk_span(text,**kw):\n",
    "        ret=etree.Element('span')\n",
    "        for k,v in kw.items(): ret.attrib[k]=v\n",
    "        ret.text=text\n",
    "        return ret\n",
    "    # print(pgno,d2[0].text.strip())\n",
    "    d1.append(mk_span(d2[0].text.strip().replace('\\n',' ')))\n",
    "    d1.attrib['subtitle_pali']=d3[1].text\n",
    "    p=d1.getparent()\n",
    "    p.remove(d2)\n",
    "    p.remove(d3)\n",
    "\n",
    "# intro sectioning\n",
    "\n",
    "for div in book.xpath('page/div[span/@family=\"smallcaps\" and (span/@size=\"12\" or span/@size=\"9\")]'):\n",
    "    #div=span.getparent()\n",
    "    pgno=int(div.getparent().attrib['id'])\n",
    "    if not 24<pgno<61: continue\n",
    "    size=div[-1].attrib['size']\n",
    "    div.tag=('heading-2-chapter' if size=='12' else 'heading-3-section')\n",
    "    print('*' if size==\"12\" else '  *',pgno,'|'.join([s.text.strip() for s in div if s.text]))\n",
    "    for s in div:\n",
    "        s.attrib.pop('size')\n",
    "        s.attrib.pop('family')\n",
    "        if s.text is not None: s.text=s.text.replace('\\n',' ')# strip()\n",
    "    #\n",
    "    #span.attrib.pop('family'); span.attrib.pop('size'); span.attrib['x']=div.attrib['x']; span.attrib['y']=div.attrib['y']\n",
    "    #div.addprevious(span)\n",
    "    #if len(div)==0: div.getparent().remove(div)\n",
    "\n",
    "#\n",
    "# tricky, some of these headings are sections and some are subsections\n",
    "# they use the same font, though, are only indented differently in the ToC (that info is lost in the XML unfortunately)\n",
    "#\n",
    "chap2page=np.array(chap2page)\n",
    "for d in book.xpath('.//div[span/@size=\"9\" and span/@family=\"smallcaps\"]'):\n",
    "    # skip stuff in TOC\n",
    "    pgno=int(d.getparent().attrib['id'])\n",
    "    chap=np.searchsorted(chap2page,pgno,side='right')\n",
    "    if pgno==63 and chap!=1: raise RuntimeError('Chapters out of sync? Chapter I is on page 63')\n",
    "    if pgno<30: continue\n",
    "    # if pgno<139 or pgno>139: continue\n",
    "    # print(pgno,len(d),'#',\"|\".join([s.text for s in d if s.text is not None])[:100])\n",
    "    heading=None\n",
    "    def _flatten(ss): return ' '.join([s.text for s in ss if s.text is not None])\n",
    "            \n",
    "            \n",
    "    if d[0].text.startswith('[') and len(d)==1:\n",
    "        heading=d[0].text.strip()\n",
    "        # away=1\n",
    "    elif m:=re.match(r'^(\\([xiv]+\\))',d[0].text):\n",
    "        print(pgno,len(d),':',\"|\".join([s.text for s in d if s.text is not None])[:100])\n",
    "        if '?' in d[0].text:\n",
    "            heading=d[0].text\n",
    "        else:\n",
    "            num=m.group(1)\n",
    "            spl=d[1].text.split('?')\n",
    "            heading=num+' '+spl[0].strip()+'?'\n",
    "            d[1].text=(spl[1] if len(spl)>1 else '')\n",
    "    elif m:=re.match(r'^(?P<para>[0-9]+\\.)\\s+(?P<num>\\([xiv]+\\))(?P<tail>.*)$',d[0].text,re.DOTALL):\n",
    "        print('##',pgno,len(d),\":\",\"|\".join([s.text for s in d if s.text is not None]).replace('\\n','|')[:100])\n",
    "        if 0:\n",
    "            if '?' in d[0].text:\n",
    "                heading=m.group('num')+' '+d[0].text.split('?')[0]            \n",
    "            else:\n",
    "                spl=d[1].text.split('?')\n",
    "                heading=m.group('num')+' '+m.group('tail')+spl[0]+'?'\n",
    "                d[1].text=(spl[1] if len(spl)>1 else '')\n",
    "    else:\n",
    "        print('??',pgno,len(d),\":\",\"|\".join([s.text for s in d if s.text is not None]).replace('\\n','|')[:100])\n",
    "        # pass\n",
    "    # print(heading)\n",
    "    if heading is not None:   \n",
    "        # determine whether this is section (level 3) or subsection (level 4)\n",
    "        # this depends on chapter number and title text\n",
    "        if 1<=chap<=6: level=3\n",
    "        elif chap in (7,): level=(3 if heading.startswith('[(') else 4)\n",
    "        elif chap in (8,9,10): level=3\n",
    "        elif chap in (11,12,13): level=(4 if (heading.startswith('[(') or (chap==13 and heading=='[General]')) else 3)\n",
    "        elif chap==14:\n",
    "            #if re.match('[ABCD]\\. ',heading): level=3\n",
    "            #elif '&' in heading: level=3\n",
    "            if heading.endswith('Aggregate]') or heading=='Materiality' or heading.startswith('[Perception,'): level=4\n",
    "            else: level=3\n",
    "        elif chap==15: level=3\n",
    "        elif chap==16: level=(4 if heading.startswith('[(') else 3)\n",
    "        elif chap==17: level=(3 if heading.startswith('[Section ') else 4)\n",
    "        elif chap==18: level=(4 if heading.startswith('[(') else 3)\n",
    "        elif chap==19: level=3\n",
    "        elif chap==20: level=(4 if heading.startswith('[(') else 3)\n",
    "        elif chap==21: level=(4 if re.match('^\\[[0-9]\\.',heading) else 3)\n",
    "        elif chap in (22,23): level=3\n",
    "        else: raise RuntimeError(f'{pgno} unhandled chapter {chap}')\n",
    "        #c=list(d)\n",
    "        #for c in d: d.remove(c)\n",
    "        d.remove(d[0])\n",
    "        if level==3: e=etree.Element('heading-3-section',x=d.attrib['x'],y=d.attrib['y'])\n",
    "        elif level==4: e=etree.Element('heading-4-subsection',x=d.attrib['x'],y=d.attrib['y'])\n",
    "        print(level*'    '+str(pgno)+' '+heading)\n",
    "        e.text=heading\n",
    "        if pgno==697 and e.text.endswith('Etc.'): e.text=e.text+']'\n",
    "        d.addprevious(e)\n",
    "\n",
    "\n",
    "   \n",
    "open('xml/book.4c.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "682522af-3482-41db-84bd-1e07ed97678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.  1\n",
      "7.  1\n",
      "8.  1\n",
      "[63:=1=2] [64:=3=4=5#6] [65:=7] [66:=8=9=10] [67:=11=12=13] [68:=14=15=16=17] [69:=18#19=20] [70:=21=22=23] [71:=24] [72:=25=26] [73:=27=28=29=30=31] [74:=32=33=34=35] [75:=36=37=38=39] [76:=40=41=42] [77:=43=44] [78:=45=46=47] [79:=48=49=50=51=52] [80:=53=54] [81:=55] [82:=56=57=58] [83:=59=60=61=62] [84:=63=64=65=66=67=68] [85:=69=70] [86:=71=72#73=74] [87:=75=76=77=78=79=80] [88:=81=82=83] [89:=84=85=86=87] [90:=88=89=90=91=92] [91:=93=94] [92:=95=96] [93:=97=98=99] [94:=100=101=102=103] [95:=104=105=106=107] [96:=108=109=110=111=112] [97:=113=114=115=116=117=118] [98:=119=120=121=122] [99:=123=124=125=126] [100:=127=128=129] [101:=130=131] [102:=132=133] [103:=134=135=136] [104:=137=138=139=140] [105:] [106:=141=142=143] [107:=144=145=146=147=148=149=150=151] [108:=152=153=154=155] [109:=156=157] [110:=158] [111:=159] [112:=160=161] [113:=1=2=3] [114:=4=5#6=7] [115:=8=9=10=11=12] [116:=13=14=15=16] [117:=17=18=19] [118:=20=21=22] [119:=23=24=25=26] [120:=27=28=29] [121:=30=31=32] [122:=33=34=35=36] [123:=37=38=39=40=41=42] [124:=43=44=45=46=47] [125:=48=49=50=51=52=53] [126:=54=55=56=57] [127:=58=59=60] [128:=61=62=63=64] [129:=65=66=67=68] [130:=69=70=71=72=73=74] [131:=75=76=77=78] [132:#79=80=81=82] [133:=83=84=85=86=87] [134:=88=89=90=91=92] [135:=93] [136:] [137:] [138:] [139:=1=2] [140:=3=4=5] [141:=6=7=8=9=10=11=12] [142:=13=14=15=16=17=18] [143:=19=20=21=22] [144:=23=24=25=26=27=28] [145:=29=30=31] [146:=32=33=34=35=36=37] [147:=38=39=40=41] [148:=42=43=44=45] [149:=46=47=48=49=50=51] [150:=52=53=54=55] [151:=56=57=58] [152:=59=60=61=62=63=64] [153:=65=66=67=68=69=70] [154:=71=72=73=74=75] [155:=76=77=78=79#80] [156:=81=82=83=84] [157:=85=86=87=88] [158:=89=90=91=92=93] [159:=94=95] [160:=96=97] [161:=98=99=100=101] [162:=102=103=104=105] [163:=106=107] [164:=108=109=110=111] [165:=112=113=114=115] [166:=116=117=118] [167:=119=120=121=122] [168:=123=124=125=126=127] [169:=128=129=130=131] [170:=132=133] [171:=1=2=3] [172:=4=5=6=7=8=9=10=11] [173:=12=13=14=15=16=17=18] [174:=19=20] [175:=21#22] [176:=23=24=25=26] [177:=27=28=29] [178:#30=31] [179:=32=33] [180:=34] [181:=35=36=37=38=39] [182:=40=41=42=43=44] [183:=45=46=47=48=49] [184:=50=51=52=53] [185:=54=55=56=57] [186:=58=59=60=61=62] [187:=63=64=65=66] [188:=67=68=69=70=71=72] [189:=73=74] [190:=75=76=77] [191:=78=79=80=81=82] [192:=83] [193:=84=85=86=87] [194:=88=89#90] [195:=91=92=93=94=95] [196:=96=97=98=99] [197:=100=101=102=103] [198:=104=105=106=107=108] [199:=109=110=111=112=113] [200:=114=115=116=117] [201:=118=119=120=121] [202:=122=123=124=125] [203:=126=127=128=129] [204:=130=131=132] [205:=133=134=135=136=137=138] [206:=139=140#141=142=143] [207:=144=145=146=147] [208:=148=149=150=151] [209:=152=153=154=155] [210:=156=157=158=159=160=161] [211:=162=163=164=165=166=167=168=169] [212:=170=171=172=173=174] [213:=175=176=177=178=179] [214:=180=181=182=183=184=185] [215:=186=187] [216:=188=189=190=191=192] [217:=193=194=195=196] [218:=197=198=199=200=201=202] [219:] [220:=1=2=3=4] [221:=5=6=7=8=9] [222:=10=11=12=13=14=15] [223:=16=17=18=19=20=21=22=23] [224:=24=25=26=27=28=29] [225:=30=31=32=33=34=35=36=37=38=39=40] [226:=41=42] [227:=1=2=3#4=5] [228:=6=7=8=9=10=11=12=13=14] [229:=15=16=17=18=19=20=21] [230:=22=23=24=25=26] [231:=27=28=29=30=31=32=33=34=35] [232:=36=37=38=39=40=41=42] [233:=43=44=45=46=47=48=49=50] [234:=51=52=53=54=55=56] [235:=57=58=59=60=61=62] [236:=63=64=65=66=67] [237:=68=69=70=71=72] [238:=73=74=75=76=77=78] [239:=79=80=81] [240:=82=83=84=85] [241:=86=87=88=89=90] [242:=91=92=93] [243:=94] [244:=1] [245:] [246:=2=3=4=5=6=7] [247:=8=9=10=11=12=13=14=15=16] [248:=17=18=19=20=21=22] [249:=23=24=25] [250:=26=27=28] [251:=29=30] [252:] [253:=31] [254:=32=33] [255:=34#35] [256:=36=37=38=39] [257:=40] [258:=41=42] [259:=43=44=45] [260:=46=47=48] [261:=49=50=51] [262:=52=53=54] [263:=55=56] [264:=57=58=59] [265:=60=61] [266:=62=63=64=65] [267:=66=67=68] [268:=69=70=71] [269:=72=73] [270:=74=75=76=77=78] [271:=79=80=81=82] [272:=83=84=85=86=87] [273:=88=89=90] [274:=91=92=93=94=95] [275:=96=97=98] [276:=99=100=101] [277:=102=103=104=105=106] [278:=107=108=109=110=111=112] [279:=113=114=115=116] [280:=117=118=119=120=121] [281:=122=123=124=125=126=127] [282:=128] [283:=1=2=3] [284:=4=5=6=7=8=9=10] [285:=11=12=13=14] [286:=15=16#17] [287:=18=19=20=21] [288:=22=23=24=25] [289:=26=27=28] [290:=29=30=31=32=33=34=35] [291:=36=37=38=39] [292:] [293:=40] [294:=41=42] [295:=43=44=45=46] [296:=47=48=49=50=51=52] [297:=53=54=55=56=57=58=59=60] [298:=61=62=63=64=65=66] [299:=67=68=69=70] [300:=71=72=73=74=75] [301:=76=77=78=79=80] [302:=81=82=83=84=85=86] [303:=87=88=89=90=91] [304:=92=93=94=95] [305:=96=97=98=99] [306:=100=101=102=103=104] [307:=105=106=107=108=109] [308:=110=111=112=113=114=115] [309:=116=117=118=119] [310:=120=121] [311:=122=123=124=125=126=127] [312:=128=129=130] [313:=131=132=133] [314:=134=135=136] [315:=137=138=139=140] [316:=141=142=143=144] [317:=145] [318:=146=147=148=149] [319:=150=151=152=153] [320:=154=155=156=157] [321:=158=159=160=161=162] [322:=163=164=165=166=167] [323:=168] [324:=169] [325:=170=171=172=173] [326:=174=175=176=177=178] [327:=179=180=181] [328:=182=183=184] [329:=185=186=187=188] [330:=189=190=191=192=193=194] [331:=195=196=197=198] [332:=199=200=201=202] [333:=203=204=205=206] [334:=207=208=209=210] [335:=211=212=213#214=215] [336:=216=217=218=219] [337:=220=221=222=223=224] [338:=225=226=227=228] [339:=229=230=231] [340:=232=233] [341:=234] [342:=235=236] [343:=237=238=239=240=241] [344:=242=243=244=245] [345:=246] [346:=247] [347:=248=249=250=251] [348:] [349:=1=2=3=4] [350:=5=6=7=8=9] [351:=10=11=12=13=14=15] [352:=16=17] [353:=18=19=20=21=22] [354:=23] [355:=24=25=26=27] [356:=28=29=30=31] [357:=32=33=34] [358:=35=36=37] [359:=38=39=40] [360:=41=42=43=44] [361:=45=46=47=48=49=50] [362:=51=52=53=54] [363:=55=56=57=58] [364:=59=60=61=62=63=64=65=66=67] [365:=68=69=70=71=72=73=74] [366:=75=76=77=78=79] [367:=80=81=82=83=84=85] [368:=86=87=88=89] [369:=90=91=92=93] [370:=94#95=96=97=98] [371:=99=100=101=102] [372:=103=104=105=106=107] [373:=108=109=110=111] [374:=112=113=114] [375:=115=116=117=118] [376:=119=120=121=122] [377:=123=124] [378:] [379:=1=2=3=4] [380:=5=6=7=8=9=10] [381:=11=12=13] [382:=14=15=16=17] [383:=18=19=20=21] [384:=22=23=24=25] [385:=26=27=28=29=30] [386:=31=32=33=34=35] [387:=36=37=38=39] [388:=40=41=42] [389:=43=44=45=46] [390:=47=48=49=50=51] [391:=52=53=54=55=56=57] [392:=58=59=60=61] [393:=62=63=64=65] [394:=66] [395:=1] [396:=2=3=4] [397:=5=6=7] [398:=8=9=10=11=12] [399:=13=14=15=16=17=18] [400:=19=20=21=22=23] [401:=24=25] [402:=26=27] [403:=28=29=30] [404:=31=32=33=34] [405:=35=36=37] [406:=38=39=40=41=42] [407:=43=44=45=46=47] [408:=48=49=50=51=52] [409:=53=54=55=56] [410:=57=58=59=60=61] [411:=62=63=64=65=66] [412:=67=68=69=70=71] [413:=72=73=74=75] [414:=76=77=78=79] [415:=80=81=82=83=84] [416:=85=86=87] [417:=88=89] [418:=90=91=92=93=94] [419:=95=96=97=98] [420:=99=100=101=102] [421:=103=104=105] [422:=106=107=108=109=110=111] [423:=112#113] [424:=114=115=116=117] [425:=118=119=120=121=122] [426:=123=124=125=126] [427:=1=2] [428:=3=4=5=6] [429:=7=8=9=10=11] [430:=12=13=14=15=16=17] [431:=18=19=20=21=22] [432:=23=24=25=26=27=28] [433:=29=30=31=32=33] [434:=34=35=36=37] [435:=38=39=40=41=42] [436:=43=44=45=46=47=48] [437:=49=50=51] [438:=52=53=54=55] [439:=56=57=58] [440:=59=60=61=62] [441:=63=64=65=66=67] [442:=68=69=70] [443:=71=72=73=74=75] [444:=76=77=78=79=80] [445:=81=82=83=84=85] [446:=86=87=88=89=90] [447:=91=92=93=94=95] [448:=96=97=98=99=100=101] [449:=102=103=104=105=106=107] [450:=108=109=110=111=112] [451:=113=114=115=116=117] [452:=118=119] [453:=120=121=122=123=124=125=126] [454:=127=128=129=130=131=132] [455:=133=134=135] [456:=136=137=138] [457:=139] [458:=1=2=3=4] [459:=5=6] [460:=7=8=9] [461:=10=11] [462:=12=13] [463:=14=15=16=17=18] [464:=19=20=21=22=23=24] [465:=25=26=27=28=29] [466:=30=31=32=33] [467:=34=35=36=37=38] [468:=39=40=41=42=43=44] [469:=45=46=47=48=49=50=51=52] [470:=53=54=55=56=57] [471:=58=59=60=61=62] [472:=63=64=65=66=67=68] [473:=69=70=71=72] [474:=73=74=75=76] [475:=77=78=79=80=81=82=83] [476:=84=85=86=87=88=89] [477:=90=91=92=93] [478:=94=95=96=97=98] [479:=99=100=101=102=103=104] [480:=105=106=107] [481:=108=109=110=111=112] [482:=113=114#115=116] [483:=117=118=119=120=121] [484:=122=123=124=125=126] [485:=127=128=129] [486:] [487:] [488:] [489:=1=2=3] [490:=4=5#6] [491:=7=8=9] [492:=10=11=12=13=14] [493:=15=16=17=18=19] [494:=20=21=22=23] [495:=24=25] [496:=26=27=28=29] [497:=30=31=32=33=34] [498:=35=36=37] [499:=38=39=40=41#42=43] [500:=44=45] [501:=46] [502:=47=48=49=50=51] [503:=52=53=54=55=56=57=58] [504:=59] [505:=60] [506:=61] [507:] [508:=62=63=64=65] [509:=66=67=68] [510:=69=70=71=72=73] [511:=74=75=76] [512:=77=78=79] [513:=80=81] [514:=82=83] [515:=84=85=86=87=88] [516:=89=90=91=92=93=94=95] [517:=96=97=98=99] [518:=100=101=102] [519:=103=104=105=106=107=108=109] [520:=110=111=112=113] [521:=114=115=116] [522:#117=118=119=120] [523:#121#122] [524:=123=124=125=126] [525:=127=128] [526:=129=130=131=132] [527:=133] [528:=134=135] [529:=136=137=138=139=140=141] [530:=142=143=144=145] [531:=146=147=148=149] [532:=150#151=152=153=154=155] [533:=156=157=158=159] [534:=160=161=162=163=164=165] [535:=166=167=168=169=170] [536:=171=172=173#174] [537:=175=176=177=178] [538:=179=180=181=182=183] [539:=184=185=186] [540:=187=188=189=190=191] [541:=192541 span\n",
      "=193=194=195=196] [542:=197=198=199=200] [543:=201=202=203=204=205] [544:=206=207=208=209=210] [545:=211=212=213=214] [546:=215=216=217] [547:=218=219=220=221] [548:=222=223=224=225=226=227=228] [549:=229=230] [550:=1=2=3] [551:=4] [552:=5=6=7=8#9=10] [553:=11=12=13=14] [554:=15=16=17] [555:=18=19=20=21] [556:=22=23=24=25] [557:=26=27=28=29=30=31] [558:=32=33=34=35] [559:=36=37=38=39=40] [560:=41=42=43] [561:=1=2=3=4] [562:=5=6=7=8] [563:=9=10=11=12] [564:=13=14=15=16=17] [565:=18=19=20=21=22=23] [566:=24=25=26] [567:=27=28=29=30=31] [568:=32=33=34] [569:=35] [570:=36=37=38=39=40] [571:=41=42=43] [572:=44=45=46=47] [573:=48=49] [574:=50=51=52=53] [575:=54=55=56] [576:=57=58=59=60=61] [577:=62=63=64=65] [578:=66=67] [579:=68=69] [580:=70=71=72=73] [581:=74] [582:=75=76=77=78] [583:=79=80=81=82=83] [584:=84=85] [585:=86] [586:=87=88=89=90] [587:=91=92] [588:=93=94=95=96=97] [589:=98=99=100=101=102] [590:=103=104] [591:=1=2=3] [592:=4=5=6=7] [593:=8=9=10=11=12] [594:=13=14=15=16] [595:=17=18=19] [596:=20=21=23=24] [597:=25] [598:=26=27=28=29=30] [599:=31=32=33=34] [600:=35=36=37=38=39=40] [601:=41=42=43=44] [602:=45=46=47=48] [603:=49=50=51] [604:=52=53=54=55] [605:=56=57=58=59] [606:=60=61=62] [607:=63=64=65=66] [608:=67=68=69=70] [609:=71=72=73] [610:=74#75=76=77] [611:=78=79=80=81] [612:=82=83=84=85] [613:=86=87=88=89=90] [614:=91=92=93=94] [615:=95=96] [616:=97=98=99=100] [617:=101=102=103=104=105] [618:=106=107=108=109] [619:=110=111] [620:=112=113=114=115=116=117=118=119] [621:=120=121=122] [622:=123=124=125=126] [623:=127=128=129] [624:=130=131=132=133] [625:=134=135=136] [626:=137=138=139=140] [627:=141=142=143] [628:=144=145=146=147=148] [629:=149=150=151=152=153=154] [630:=155=156=157=158=159=160] [631:=161=162=163] [632:=164=165=166=167=168=169] [633:=170=171=172=173=174] [634:=175=176=177=178] [635:=179=180=181=182] [636:=183=184=185] [637:=186=187=188=189=190] [638:=191=192=193=194] [639:=195=196=197] [640:=198=199=200=201=202] [641:=203=204=205] [642:=206=207=208=209=210=211] [643:=212=213=214=215=216=217] [644:=218=219=220=221=222=223] [645:=224=225=226=227] [646:=228=229=230=231=232] [647:=233=234=235=236=237=238] [648:=239=240=241] [649:=242=243=244] [650:=245=246=247=248] [651:=249=250=251=252=253=254] [652:=255=256=257=258] [653:=259=260=261=262] [654:=263=264=265=266=267] [655:=268=269=270=271] [656:=272=273=274=275=276=277=278] [657:=279=280=281=282=283] [658:=284=285=286=287=288=289] [659:=290=291=292=293=294] [660:=295=296] [661:=297=298=299=300=301] [662:=302=303] [663:=304=305=306=307] [664:=308=309] [665:=310=311] [666:=312=313=314] [667:=1=2=3] [668:=4=5] [669:=6=7=8] [670:=9=10=11=12] [671:=13=14] [672:=15=16=17=18=19] [673:=20=21=22=23] [674:=24=25] [675:=26=27=28=29=30] [676:=31=32=33] [677:=34=35] [678:=36=37] [679:=1=2=3=4] [680:=5=6=7=8=9] [681:=10=11=12=13] [682:=14] [683:=15=16] [684:=17=18] [685:=19=20] [686:=21=22] [687:=23=24=25=26] [688:=27] [689:=1=2=3] [690:=4] [691:=5=6=7] [692:=8=9] [693:=10=11=12=13] [694:=14=15=16=17] [695:=18=19] [696:=20] [697:=21=22=23=24] [698:=25=26=27=28=29] [699:=30=31=32] [700:=33=34=35=36=37] [701:=38=39=40=41=42] [702:=43=44] [703:=45=46=47] [704:=48=49=50=51=52] [705:=53=54=55=56=57=58=59] [706:=60=61=62=63=64=65] [707:=66=67=68] [708:=69=70=71=72] [709:=73=74=75] [710:=76#77=78=79] [711:=80=81=82] [712:=83=84=85=86=87=88=89] [713:=90] [714:=91] [715:=92=93=94=95] [716:=96=97] [717:=98=99=100=101=102] [718:=103=104=105] [719:=106#107] [720:=108=109=110=111=112] [721:=113=114=115=116=117=118=119=120] [722:=121=122=123=124=125] [723:=126=127=128=129=130] [724:=1=2] [725:=3=4] [726:=5=6=7=8=9=10] [727:=11] [728:=12=13=14=15] [729:=16=17=18=19=20=21] [730:=22=23=24=25=26] [731:=27=28=29=30] [732:=31=32=33=34] [733:=35=36=37] [734:] [735:=38=39=40=41] [736:=42=43=44] [737:=45=46=47=48] [738:=49=50=51] [739:=52=53] [740:=54=55=56] [741:=57=58=59#60] [742:=61=62=63=64=65] [743:=66=67] [744:=68=69=70=71=72=73] [745:] [746:=74=75=76] [747:=77=78=79=80=81=82] [748:=83=84] [749:=85=86=87=88=89] [750:=90=91=92] [751:=93=94=95=96=97=98=99] [752:=100=101=102=103=104=105=106=107=108=109] [753:=110=111=112=113=114] [754:=115=116=117=118=119] [755:=120=121=122=123=124=125#126] [756:=127=128=129=130] [757:=131=132=133=134=135] [758:=136] [759:=1=2=3=4=5] [760:=6] [761:=7=8=9=10=11#12=13] [762:=14=15=16=17=18=19] [763:=20=21=22=23] [764:=24=25=26=27=28=29] [765:=30=31=32=33] [766:=34=35=36=37] [767:=38=39] [768:=40=41=42=43=44] [769:=45] [770:=46=47=48] [771:=49=50=51=52=53=54=55=56] [772:=57=58=59=60=61=62=63=64] [773:=65=66=67=68=69=70=71] [774:=72=73=74=75=76=77=78] [775:=79] [776:=80=81=82=83] [777:=84=85=86=87=88] [778:=89=90=91] [779:=92=93=94=95=96] [780:=97=98=99] [781:=100=101=102=103=104=105=106] [782:=107=108=109=110=111=112] [783:=113=114] [784:=115=116=117=118=119] [785:=120=121=122=123=124=125] [786:=126=127=128=129] [787:] [788:=1=2=3] [789:#4=5=6=7] [790:=8=9=10] [791:#11=12=13=14] [792:=15=16=17] [793:=18=19=20=21] [794:=22=23=24=25=26] [795:=27=28] [796:=29=30=31=32=33=34] [797:=35=36=37=38] [798:=39=40=41=42=43] [799:=44=45=46=47=48=49=50] [800:=51=52=53=54] [801:=55=56=57=58=59=60] [802:] "
     ]
    },
    {
     "data": {
      "text/plain": [
       "3771573"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## PARAGRAPHS\n",
    "##\n",
    "\n",
    "# now we have chapters so can confidently know where paragraphs reset\n",
    "book=tree=etree.parse('xml/book.4c.xml',etree.XMLParser()).getroot()\n",
    "# open('vism.book.5.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))\n",
    "\n",
    "# pg 654, missing paragraph number\n",
    "## HACK: missing paragraph numbers\n",
    "##\n",
    "for prefix,txt in [\n",
    "        ('87. ','Thus there comes to be the removal of [false] view'), # pg654\n",
    "        ('7. ','impermanent  in  the  sense  of  destruction,  painful'), # pg692\n",
    "        ('8. ','Understanding of defining by summarization thus, ‘With birth'), # pg962\n",
    "    ]:\n",
    "    fix0=book.xpath('.//div[contains(span,\"'+txt+'\")]')\n",
    "    print(prefix,len(fix0))\n",
    "    assert(len(fix0)==1)\n",
    "    fix0[0][0].text=prefix+fix0[0][0].text\n",
    "\n",
    "\n",
    "nextPar=None\n",
    "currChap=None\n",
    "for page in list(book):\n",
    "    pgno=int(page.attrib['id'])\n",
    "    if pgno<63: continue\n",
    "    # if pgno>64: continue\n",
    "    if pgno>802: break\n",
    "    print(f'[{pgno}:',end=\"\")\n",
    "    for d in page:\n",
    "        if d.tag=='heading-2-chapter':\n",
    "            nextPar=1\n",
    "            currChap=d.attrib['toc_num']\n",
    "            continue\n",
    "        if d.tag!='div' and d.tag!='verse' and not d.tag.startswith('heading-'): print(f'{pgno} {d.tag}')\n",
    "        if pgno==491 and int(d.attrib['x'])>60: continue\n",
    "        if pgno==692 and nextPar==10: continue \n",
    "        while True:\n",
    "            found=False\n",
    "            pat=re.compile(r'^\\s*'+str(nextPar)+r'(\\.|\\s|$)(?P<tail>.*)$',re.DOTALL)\n",
    "            for s in d:\n",
    "                if s.text is None: continue\n",
    "                #if pgno==64: print(s.text,nextPar,)\n",
    "                # forgotten in the PDF, found in the older paper edition\n",
    "                if pgno==596 and nextPar==22 and re.match('The\\s+first',s.text):\n",
    "                    # print(f'@HACK@{nextPar}',end=\"\")\n",
    "                    s.addprevious(e:=etree.Element('vism-para',num=str(nextPar),anchor=currChap+'.'+str(nextPar)))\n",
    "                    # s.text=str(nextPar)+'. '+s.text\n",
    "                    e.text=str(nextPar)\n",
    "                    nextPar+=1\n",
    "                    found=True\n",
    "                # one case (pg 98, §122) forgot the dot, so try both with and without\n",
    "                elif m:=pat.match(s.text): # ,s.text.startswith(str(nextPar)+'. ') or s.text.startswith(str(nextPar)+' '): \n",
    "                    print(f'={nextPar}',end=\"\")\n",
    "                    s.text=m.group('tail').lstrip()\n",
    "                    # if s[0]==' ': s=s[1:]\n",
    "                    s.addprevious(e:=etree.Element('vism-para',num=str(nextPar),anchor=currChap+'.'+str(nextPar)))\n",
    "                    e.text=str(nextPar)\n",
    "                    # XX.42: has dot extra as italics, remove it\n",
    "                    if s.text=='': s.getparent().remove(s)\n",
    "                    if ((n:=e.getnext()) is not None) and n.text=='. ': n.getparent().remove(n)\n",
    "                    nextPar+=1\n",
    "                    found=True\n",
    "            if found: continue\n",
    "            # if not found at the beginning of the span, search inside the span, as a separate line\n",
    "            for s in d:\n",
    "                if s.text is None: continue\n",
    "                if len(sp:=s.text.split('\\n'+str(nextPar)+'.'))>1:\n",
    "                    assert len(sp)==2\n",
    "                    print(f'#{nextPar}',end=\"\")\n",
    "                    s.addprevious(sLeft:=copy.deepcopy(s))\n",
    "                    sLeft.text=sp[0]\n",
    "                    s.text=sp[1]\n",
    "                    s.addprevious(e:=etree.Element('vism-para',num=str(nextPar),anchor=currChap+'.'+str(nextPar)))\n",
    "                    e.text=str(nextPar)\n",
    "                    nextPar+=1\n",
    "                    found=True\n",
    "            if not found: break\n",
    "    # add anchors for footnotes in this chapter\n",
    "    for fn in page.findall('.//footnote'):\n",
    "        # a=etree.Element(',\n",
    "        #fn.insert(0\n",
    "        fn.attrib['anchor']=f'{currChap}.n{fn.attrib[\"mark\"]}'\n",
    "        \n",
    "        \n",
    "    print(']',end=\" \")\n",
    "\n",
    "open('xml/book.5.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "044a8596-f660-4c6c-bd40-9df628efbe2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................[1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] [48] [49] [50] [51] [52] [53] [54] [55] [56] [57] [58] [59] [60] [61] [62] [63] [64] [65] [66] [67] [68] [69] [70] [71] [72] [73] [74] [75] [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] [91] [92] [93] [94] [95] [96] [97] [98] [99] [100] [101] [102] [103] [104] [105] [106] [107] [108] [109] [110] [111] [112] [113] [114] [115] [116] [117] [118] [119] [120] [121] [122] [123] [124] [125] [126] [127] [128] [129] [130] [131] [132] [133] [134] [135] [136] [137] [138] [139] [140] [141] [142] [143] [144] [145] [146] [147] [148] [149] [150] [151] [152] [153] [154] [155] [156] [157] [158] [159] [160] [161] [162] [163] [164] [165] [166] [167] [168] [169] [170] [171] [172] [173] [174] [175] [176] [177] [178] [179] [180] [181] [182] [183] [184] [185] [186] [187] [188] [189] [190] [191] [192] [193] [194] [195] [196] [197] [198] [199] [200] [201] [202] [203] [204] [205] [206] [207] [208] [209] [210] [211] [212] [213] [214] [215] [216] [217] [218] [219] [220] [221] [222] [223] [224] [225] [226] [227] [228] [229] [230] [231] [232] [233] [234] [235] [236] [237] [238] [239] [240] [241] [242] [243] [244] [245] [246] [247] [248] [249] [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] [262] [263] [264] [265] [266] [267] [268] [269] [270] [271] [272] [273] [274] [275] [276] [277] [278] [279] [280] [281] [282] [283] [284] [285] [286] [287] [288] [289] [290] [291] [292] [293] [294] [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] [305] [306] [307] [308] [309] [310] [311] [312] [313] [314] [315] [316] [317] [318] [319] [320] [321] [322] [323] [324] [325] [326] [327] [328] [329] [330] [331] [332] [333] [334] [335] [336] [337] [338] [339] [340] [341] [342] [343] [344] [345] [346] [347] [348] [349] [350] [351] [352] [353] [354] [355] [356] [357] [358] [359] [360] [361] [362] [363] [364] [365] [366] [367] [368] [369] [370] [371] [372] [373] [374] [375] [376] [377] [378] [379] [380] [381] [382] [383] [384] [385] [386] [387] [388] [389] [390] [391] [392] [393] [394] [395] [396] [397] [398] [399] [400] [401] [402] [403] [404] [405] [406] [407] [408] [409] [410] [411] [412] [413] [414] [415] [416] [417] [418] [419] [420] [421] [422] [423] [424] [425] [426] [427] [428] [429] [430] [431] [432] [433] [434] [435] [436] [437] [438] [439] [440] [441] [442] [443] [444] [445] [446] [447] [448] [449] [450] [451] [452] [453] [454] [455] [456] [457] [458] [459] [460] [461] [462] [463] [464] [465] [466] [467] [468] [469] [470] [471] [472] [473] [474] [475] [476] [477] [478] [479] [480] [481] [482] [483] [484] [485] [486] [487] [488] [489] [490] [491] [492] [493] [494] [495] [496] [497] [498] [499] [500] [501] [502] [505] [506] [507] [508] [509] [510] [511] [512] [513] [514] [515] [516] [517] [518] [519] [520] [521] [522] [523] [524] [525] [526] [527] [528] [529] [530] [531] [532] [533] [534] [535] [536] [537] [538] [539] [540] [541] [542] [543] [544] [545] [546] [547] [548] [549] [550] [551] [552] [553] [554] [555] [556] [557] [558] [559] [560] [561] [562] [563] [564] [565] [566] [567] [568] [569] [570] [571] [572] [573] [574] [575] [576] [577] [578] [579] [580] [581] [582] [583] [584] [585] [586] [587] [588] [589] [590] [591] [592] [593] [594] [595] [596] [597] [598] [599] [600] [601] [602] [603] [604] [605] [606] [607] [608] [609] [610] [611] [612] [613] [614] [615] [616] [617] [618] [619] [620] [621] [622] [623] [624] [625] [626] [627] [628] [629] [630] [631] [632] [633] [634] [635] [636] [637] [638] [639] [640] [641] [642] [643] [644] [645] [646] [647] [648] [649] [650] [651] [652] [653] [654] [655] [656] [657] [658] [659] [660] [661] [662] [663] [664] [665] [666] [667] [668] [669] [670] [671] [672] [673] [674] [675] [676] [677] [678] [679] [680] [681] [682] [683] [684] [685] [686] [687] [688] [689] [690] [691] [692] [693] [694] [695] [696] [697] [698] [699] [700] [701] [702] [703] [704] [705] [706] [707] [708] [709] [710] 400 no section yet! [(2) The Divine Ear Element]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "299129"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## LOGICAL STRUCTURE:\n",
    "## - book, index, glossary as separate documents\n",
    "## - \n",
    "##\n",
    "from lxml import etree\n",
    "import itertools,copy,re\n",
    "\n",
    "book=tree=etree.parse('xml/book.5.xml',etree.XMLParser(remove_blank_text=True)).getroot() # remove_blank_text=True)).getroot()\n",
    "index=etree.Element(\"index\")\n",
    "##\n",
    "## get rid of pages, structure everything logically\n",
    "##\n",
    "for page in list(book):\n",
    "    pgno=int(page.attrib['id'])\n",
    "    assert page.tag=='page'\n",
    "    if len(page)==0 or pgno<25 or pgno>806:\n",
    "        # separate output for index and glossary\n",
    "        if pgno>=809:\n",
    "            for i,d in enumerate(page):\n",
    "                # drop headers for index and glossary\n",
    "                if pgno in (809,832) and i in (0,1,2): continue \n",
    "                d.attrib['page_id']=page.attrib['id']\n",
    "                index.append(d)\n",
    "        book.remove(page)\n",
    "        continue\n",
    "    for d in page:\n",
    "        d.attrib['page_id']=page.attrib['id']\n",
    "        d.attrib['page_no']=page.attrib.get('pageno','N/A')\n",
    "    if 'pageno' in page.attrib: \n",
    "        e=page[0]\n",
    "        while e.tag.startswith('heading-'): e=e.getnext()\n",
    "        e.insert(0,br:=etree.Element('printed_page',edition='BPS2011',page_id=d.attrib['page_id']))\n",
    "        br.text=page.attrib['pageno']\n",
    "    if 'continuation' in page[0].attrib:\n",
    "        # print(pgno,book[-1].tag)\n",
    "        if book[-1].tag=='div':\n",
    "            for s in page[0]: book[-1].append(s)\n",
    "            # print(f'{pgno} {page[0].tag} {len(page[0])} {page[0].text}')\n",
    "            assert len(page[0])==0 and (page[0].text is None or page[0].text.strip()=='')\n",
    "            page.remove(page[0])\n",
    "        else: book.append(page[0])\n",
    "        print('.',end='')\n",
    "    # assert 'continuation' not in page[0]\n",
    "    for d in page: book.append(d)\n",
    "    assert len(page)==0\n",
    "    book.remove(page)\n",
    "front=etree.Element('heading-1-part',toc_name='(Front)')\n",
    "front.text='(Front)'\n",
    "book.insert(0,front)\n",
    "\n",
    "def _splitSpan(span,headText,mids,tailText):\n",
    "    if len(headText)==0: # don't prepend empty span\n",
    "        for e in mids: span.addprevious(e)\n",
    "        span.text=tailText\n",
    "    elif len(tailText)==0: # don't append empty span\n",
    "        for e in mids: span.addnext(e)\n",
    "        span.text=headText\n",
    "    else: # split\n",
    "        s0=copy.deepcopy(span)\n",
    "        s0.text=headText\n",
    "        span.addprevious(s0)\n",
    "        for e in mids: span.addprevious(e)\n",
    "        span.text=tailText\n",
    "\n",
    "# locate PTS page marks [25] etc in all spans (includes nested in verse etc), sequentially\n",
    "pagePat=re.compile(r'^(?P<head>.*)\\[(?P<num>[0-9]{1,3})\\](?P<tail>.*)$',re.DOTALL)\n",
    "for span in book.findall('.//span'):\n",
    "    if span.text is None: continue\n",
    "    while m:=pagePat.match(span.text):\n",
    "        head,num,tail=m.group('head'),m.group('num'),m.group('tail')\n",
    "        pg=etree.Element('printed_page',edition='PTS')\n",
    "        pg.text=num\n",
    "        _splitSpan(span,head,[pg],tail)\n",
    "        print(f'[{num}]',end=' ')\n",
    "        \n",
    "## create section tree\n",
    "currSect=[]\n",
    "for e in book:\n",
    "    if e.tag=='heading-1-part':\n",
    "        p=etree.Element('struct-1-part',name=e.attrib['toc_name'])\n",
    "        # p.text=e.text\n",
    "        book.insert(book.index(e),p)\n",
    "        currSect=[p]\n",
    "    elif e.tag=='heading-2-chapter':\n",
    "        p=etree.Element('struct-2-chapter',name=(e.get('toc_name',' '.join([s.text for s in e if s.text]))))\n",
    "        # p.text=e.text\n",
    "        currSect[0].append(p)\n",
    "        currSect=[currSect[0],p]\n",
    "    elif e.tag=='heading-3-section':\n",
    "        p=etree.Element('struct-3-section',name=(e.text if e.text else ' '.join([s.text for s in e if s.text is not None])))\n",
    "        # p.text=e.text\n",
    "        # print(e.attrib['page_no'])\n",
    "        # print(len(currSect))\n",
    "        if len(currSect)<=1:\n",
    "            print(e.attrib['page_no'],'no chapter yet!',e.text)\n",
    "            c=etree.Element('struct-2-chapter',name='[fake-chapter]')\n",
    "            c.text='[fake chapter]'\n",
    "            currSect[0].append(c)\n",
    "            currSect.append(c)\n",
    "        currSect[1].append(p)\n",
    "        currSect=[currSect[0],currSect[1],p]\n",
    "    elif e.tag=='heading-4-subsection':\n",
    "        p=etree.Element('struct-4-subsection',name=(e.text if e.text else ' '.join([s.text for s in e if s.text is not None])))\n",
    "        if len(currSect)<=2:\n",
    "            print(e.attrib['page_no'],'no section yet!',e.text)\n",
    "            c=etree.Element('struct-3-section',name='[fake-section]')\n",
    "            currSect[1].append(c)\n",
    "            currSect.append(c)\n",
    "        currSect[2].append(p)\n",
    "        currSect=[currSect[0],currSect[1],currSect[2],p]\n",
    "    currSect[-1].append(e)\n",
    "\n",
    "## fuse paragraph tails\n",
    "for d1 in book.findall('.//div'):\n",
    "    if len(d1)!=1 or d1[0].tag!='span' or d1[0].text is None: continue\n",
    "    if (d0:=d1.getprevious()) is None or len(d0)<1 or d0[-1].tag!='span' or d0[-1].text is None: continue\n",
    "    s1,s0=d1[0],d0[-1]\n",
    "    t1,t0=s1.text.strip(),s0.text.strip()\n",
    "    #if len(s1)==0: continue\n",
    "    if len(t1)>=100: continue\n",
    "    if (not t0.endswith('.')) and t1[0].islower() and (t1.endswith('.') or t1.endswith('?')):\n",
    "        # print(t1)\n",
    "        s0.text+=s1.text\n",
    "        d1.remove(s1)\n",
    "    # print(s1.text)\n",
    "    \n",
    "    \n",
    "\n",
    "open('xml/book.5a.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))\n",
    "open('xml/index.0.xml','w').write(etree.tostring(index,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dc352c85-6393-4442-b69d-87eef88d75a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXI.10, 27, 37, n.6; XXII.5, 44, 79; XXI-II.7\n",
      "ascetic practice ( False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59099"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## INDEX+GLOSSARY (1): fix markup, split to entries\n",
    "##\n",
    "\n",
    "index=tree=etree.parse('xml/index.0.xml',etree.XMLParser()).getroot() \n",
    "\n",
    "# improper spacing from pdf export, fix that\n",
    "if 1:\n",
    "    for e in index.xpath('.//span[contains(text(),\"c i t t a\")]'): e.text=e.text.replace(' ','')\n",
    "    for e in index.xpath('.//span[contains(text(),\"k n o c k i n g \")]'): e.text=e.text.replace(' ','')\n",
    "\n",
    "# remove hyphenation\n",
    "patHyph=re.compile('(?<=\\w)-\\n(?=\\w)')\n",
    "# patHyph=re.compile('\\w-\\n\\w')\n",
    "for e in index.findall('.//span'):\n",
    "    if e.text is None: continue\n",
    "    e.text,n=patHyph.subn('',e.text)\n",
    "    # HACK\n",
    "    e.text=e.text.replace(\"XIVn.27\",\"XIV.n27\").replace('XXI–','XXI-')\n",
    "    \n",
    "# remove heading: sort everything\n",
    "def ix_key(k):\n",
    "    # page,column,vertical position\n",
    "    return int(k.attrib['page_id']),(1 if int(k.attrib['x'])<100 else 2),int(k.attrib['y'])\n",
    "index[:]=sorted(index,key=ix_key)\n",
    "# fix offset left/right\n",
    "for d in index:\n",
    "    if int(d.attrib['page_id'])%2==0: d.attrib['x']=str(int(d.attrib['x'])+6)\n",
    "# is this conuation?\n",
    "# X is a hack for one case...\n",
    "def _isCont(d): return (52<(x:=int(d.attrib['x']))<58) or (231<x<236) or (len(d)>0 and (d[0].text in ('XV') or d[0].text.startswith('95')))\n",
    "\n",
    "index2=etree.Element(\"list\",title='Index')\n",
    "glossary=etree.Element(\"list\",title='Glossary')\n",
    "\n",
    "prevPgno=None\n",
    "prev=None\n",
    "for d in index:\n",
    "    pgno=int(d.attrib['page_id'])\n",
    "    if pgno==832 and prevPgno!=832:\n",
    "        index2.append(prev)\n",
    "        prev=None\n",
    "    prevPgno=pgno\n",
    "    if prev is None: prev=etree.Element('entry',page_id=d.attrib['page_id'])\n",
    "    if not _isCont(d):\n",
    "        if prev is not None and len(prev)>0: (index2 if pgno<832 else glossary).append(prev)\n",
    "        prev=etree.Element('entry',page_id=d.attrib['page_id'])\n",
    "    if d.attrib['y']=='54' and d.attrib['x']=='226' and d.attrib['page_id']=='810':\n",
    "        print(d[0].text,_isCont(d))\n",
    "    for e in d:\n",
    "        prev.append(e)\n",
    "if prev is not None: index.append(prev)\n",
    "\n",
    "\n",
    "open('xml/index.1.xml','w').write(etree.tostring(index2,encoding='unicode',pretty_print=True))\n",
    "open('xml/gloss.1.xml','w').write(etree.tostring(glossary,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b2a8afa4-ae1e-47ac-a1d9-6f5a80a8484f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A prompted, prompting (\n",
      "index: 965 entries\n",
      "glossary: 920 entries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92534"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## INDEX+GLOSSARY (2): split fused entries, parse entries to have title+text\n",
    "##\n",
    "\n",
    "\n",
    "index=tree=etree.parse('xml/index.1.xml',etree.XMLParser()).getroot() \n",
    "glossary=tree=etree.parse('xml/gloss.1.xml',etree.XMLParser()).getroot() \n",
    "def splitentry2(e,span,iline,dbg=False):\n",
    "    ispan=e.index(span)\n",
    "    ll=span.text.split('\\n')\n",
    "    s1=etree.Element('span')\n",
    "    s1.text='\\n'.join(ll[iline:])\n",
    "    span.text='\\n'.join(ll[:iline])\n",
    "    if dbg: print(ll[:iline],ll[iline:])\n",
    "    e1=etree.Element('entry',page_id=e.attrib['page_id'])\n",
    "    e1.append(s1)\n",
    "    for s in e[ispan+1:]: e1.append(s)\n",
    "    e.addnext(e1)\n",
    "    if span.text=='' and len(e)==1: \n",
    "        assert e==span.getparent()\n",
    "        e.getparent().remove(e)\n",
    "    \n",
    "from unidecode import unidecode\n",
    "\n",
    "for INDEX in (index,glossary):\n",
    "    for i in range(15):\n",
    "        for entry in INDEX:\n",
    "            if entry.getnext() is None or len(entry.getnext())==0: continue\n",
    "            for span in entry:\n",
    "                if span.tag!='span': continue\n",
    "                doBreak=False\n",
    "                for iline,l in enumerate(ll:=span.text.split('\\n')):\n",
    "                    if l.strip()=='': continue\n",
    "                    w0=unidecode(l.split()[0]).lower()\n",
    "                    SPLIT=False\n",
    "                    if INDEX==index:                        \n",
    "                        if w0[0] in ('*“'): w0=w0[1:]\n",
    "                        # alphabetically between start of this entry and beginning of the next one\n",
    "                        wPrev,wNext=unidecode(entry[0].text).lower(),unidecode(entry.getnext()[0].text).lower()\n",
    "                        # if entry[0].text.startswith('desir'): print('   ',l,'|',wPrev[:15],'..',wNext[:15],wPrev<w0<wNext,entry[0].text[:15])\n",
    "                        SPLIT=(wPrev<w0<wNext)\n",
    "                        dbg=False\n",
    "                    else:\n",
    "                        # in the glossary\n",
    "                        if w0.startswith('*'): SPLIT=True\n",
    "                        elif re.match('^[a-zA-Z-]+--',w0): SPLIT=True # unidecode normalizes em-dash to --\n",
    "                        dbg=False\n",
    "                        # print(w0,SPLIT)\n",
    "                    if SPLIT and not (span==entry[0] and iline==0):\n",
    "                        # print(wPrev[:10],l,wNext[:10])\n",
    "                        splitentry2(entry,span,iline,dbg=dbg)\n",
    "                        doBreak=True\n",
    "                        break\n",
    "                if doBreak: break\n",
    "    for entry in INDEX:\n",
    "        for s in entry:\n",
    "            s.text=s.text.replace('\\n',' ').replace('  ',' ').replace('  ',' ')\n",
    "            if 'size' in s.attrib: del s.attrib['size']\n",
    "            # if s.tail and s.tail.strip()=='': s.tail='' ##### XXXXX???\n",
    "            # if s.tail.strip()!='': print('$')\n",
    "        entry.tail='\\n  '\n",
    "\n",
    "open('xml/index.1a.xml','w').write(etree.tostring(index,encoding='unicode',pretty_print=True))\n",
    "        \n",
    "# split entries into title and text\n",
    "for ent in index:\n",
    "    # continue\n",
    "    s0=ent[0]\n",
    "    if 'prompted' in s0.text: print('A',s0.text)\n",
    "    t=s0.text\n",
    "    # if there is chaptere reference just after the word, only match for entry whatever precedes\n",
    "    if chapref:=re.search(r'[XIV]{2,}',t): t=t[:chapref.span()[0]]\n",
    "    m=re.match(r'^(?P<title>[\\w\\s’-]+)((,|;| \\(|[XIV]+\\.|).*$)',t)\n",
    "    if m is None: ent.attrib['title']='[???]'\n",
    "    else:\n",
    "        ent.attrib['title']=m.group('title').strip()\n",
    "        # print(m.group('title'),'|',m.group('tail'))\n",
    "        s0.text=s0.text[len(m.group('title')):].strip()\n",
    "        if s0.text=='': s0.getparent().remove(s0)\n",
    "\n",
    "        \n",
    "# print(len(glossary))\n",
    "for ent in glossary:\n",
    "    s0=ent[0]\n",
    "    if '—' not in s0.text: ent.attrib['title']='[???]'\n",
    "    else: ent.attrib['title'],s0.text=s0.text.split('—',maxsplit=1)\n",
    "        \n",
    "    \n",
    "#        \n",
    "print(f'index: {len(index)} entries')\n",
    "print(f'glossary: {len(glossary)} entries')\n",
    "open('xml/index.2.xml','w').write(etree.tostring(index,encoding='unicode',pretty_print=True))\n",
    "open('xml/gloss.2.xml','w').write(etree.tostring(glossary,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "13cd7201-8b68-4f52-b79d-060b746f2430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================book\n",
      "  para 394\n",
      "  chap 42\n",
      "  bib  1973\n",
      "index\n",
      "  para 6400\n",
      "  chap 0\n",
      "  bib  0\n",
      "glossary\n",
      "  para 345\n",
      "  chap 0\n",
      "  bib  41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106940"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## find internal references (for book, index and glossary)\n",
    "##\n",
    "from lxml import etree\n",
    "import re, itertools\n",
    "\n",
    "index=tree=etree.parse('xml/index.2.xml',etree.XMLParser()).getroot() \n",
    "glossary=tree=etree.parse('xml/gloss.2.xml',etree.XMLParser()).getroot() \n",
    "book=tree=etree.parse('xml/book.5a.xml',etree.XMLParser(remove_blank_text=True)).getroot()\n",
    "\n",
    "# HACK\n",
    "for s in book.xpath('.//span[contains(text(),\"Ch. III n. 5\")]'):\n",
    "    s.text.replace('Ch. III n. 5','Ch. III, n. 5')\n",
    "\n",
    "for TOP in index,book,glossary:\n",
    "    repl=-1\n",
    "    while repl!=0:\n",
    "        repl=0\n",
    "        ## fuse adjacent spans without attributes\n",
    "        for s in TOP.findall('.//span'):\n",
    "            # if len(s.attrib)>0: continue\n",
    "            if s.getparent() is None or (s1:=s.getnext()) is None or s1.tag!='span': continue # might be a span we just removed\n",
    "            #if len(s.attrib)!=len(s1.attrib): continue\n",
    "            if s.attrib.get('family','')!=s1.attrib.get('family',''): continue\n",
    "            if s.attrib.get('size','9')!=s1.attrib.get('size','9'): continue\n",
    "            if s.text is None: continue\n",
    "            # s.text=s.text.replace('  ',' ').replace('  ',' ')\n",
    "            if s.text is None or s1.text is None: continue\n",
    "            s.text+=' '+s1.text ### XXX? unwanted spaces?\n",
    "            s.text=s.text.replace('  ',' ')\n",
    "            s.getparent().remove(s1)\n",
    "            repl+=1\n",
    "            print('=',end='')\n",
    "\n",
    "        ## fuse interleaved-family spans where the mid-span no letters (only punctuation)\n",
    "        for s0 in TOP.findall('.//span'):\n",
    "            if s0.getparent() is None: continue\n",
    "            if (s1:=s0.getnext()) is None: continue\n",
    "            if (s2:=s1.getnext()) is None: continue\n",
    "            if s.text is None or s1.text is None or s2.text is None: continue\n",
    "            def _it(s): return s.attrib.get('family','normal')=='italic'\n",
    "            ii=_it(s0),_it(s1),_it(s2)\n",
    "            alpha1=sum([c.isalnum() for c in s1.text])\n",
    "            if alpha1>0 or ')' in s1.text or '(' in s1.text: continue\n",
    "            if ii in [(0,1,0),(1,0,1)]:\n",
    "                # print('=-=' if ii[0]==1 else '-=-',s0.text[-10:],'|',s1.text,'|',s2.text[:10])\n",
    "                s0.text=(s0.text+s1.text+s2.text).replace('  ',' ')\n",
    "                s1.getparent().remove(s1)\n",
    "                s2.getparent().remove(s2)\n",
    "                repl+=1\n",
    "    \n",
    "\n",
    "# open('xml/index.2a.xml','w').write(etree.tostring(index,encoding='unicode',pretty_print=True))\n",
    "\n",
    "def _matchHeadTail(m):\n",
    "    return m.string[:m.span()[0]],m.string[m.span()[1]:]\n",
    "\n",
    "            \n",
    "def mkVismRef(*,target,text,type='vism',loc=None):\n",
    "    e=etree.Element('ref',type=type,target=target)\n",
    "    if loc is not None: e.attrib['loc']=loc\n",
    "    if text=='' or text is None: raise RuntimeError(e)\n",
    "    e.text=text.replace('\\n',' ')\n",
    "    return e\n",
    "\n",
    "# references to chapter . paragraph (possibly chained)\n",
    "def _mkXrefChapPar_2(top):\n",
    "    import re\n",
    "    patRefStart=re.compile(r'''\n",
    "        \\b(?P<chapter>\n",
    "            ((?P<ch0>[IVX]{1,5})\\.)\n",
    "            |\n",
    "            ((Ch.|Chapter)\\s+(?P<ch1>[IVX]{1,5}),?\\s+)\n",
    "            |\n",
    "            ((?P<ch2>[IVX]{1,5})\\s+(?=passim))\n",
    "        )\n",
    "    ''',re.X)\n",
    "    patPara=re.compile(r'§?(?P<num>[0-9]+)(f\\.|ff\\.)?')\n",
    "    patNote=re.compile(r'(note |n. |n.)(?P<num>[0-9]+)\\b')\n",
    "    patPassim=re.compile(r'passim\\b')\n",
    "    patCont=re.compile(r'(and\\s+|,\\s+|-|–)\\b')\n",
    "    retRefs=0\n",
    "    for span in top.findall('.//span'):\n",
    "        if span.text is None: continue\n",
    "        pgno=int(span.getparent().attrib.get('page_id',\"0\"))\n",
    "        #dbg=(pgno==812 and (entry:=span.getparent()).tag=='entry' and entry.attrib['title']=='concentration')\n",
    "        dbg=False\n",
    "        chPos0=0\n",
    "        while chapm:=patRefStart.search(span.text,pos=chPos0):\n",
    "            chap=[chapm.group(g) for g in ['ch0','ch1','ch2'] if chapm.group(g) is not None][0]\n",
    "            head,tail=_matchHeadTail(chapm)\n",
    "            chPos0=chapm.span()[1]\n",
    "            for nth in itertools.count():\n",
    "                if nth==0: pass\n",
    "                elif mc:=patCont.match(span.text):\n",
    "                    head,tail=span.text[:mc.span()[1]],span.text[mc.span()[1]:]\n",
    "                    if dbg: print(f'  {nth} continuation: {head=} {tail=}')\n",
    "                else:\n",
    "                    if dbg: print(f'  {nth} NO continuation: {tail=}')\n",
    "                    break\n",
    "                if m0:=patPara.match(tail):\n",
    "                    if dbg: print(f'  {nth} match: § {tail=}')\n",
    "                    mm,target=m0,f'{chap}.{m0.group(\"num\")}'\n",
    "                elif m1:=patNote.match(tail):\n",
    "                    if dbg: print(f'  {nth} match: N {tail=}')\n",
    "                    mm,target=m1,f'{chap}.n{m1.group(\"num\")}'\n",
    "                elif m2:=patPassim.match(tail):\n",
    "                    if dbg: print(f'  {nth} match: passim {tail=}')\n",
    "                    mm,target=m2,f'{chap}'\n",
    "                else:\n",
    "                    if dbg: print(f'  NO match: {tail=}')\n",
    "                    break\n",
    "                e=mkVismRef(text=(chapm.group(0) if nth==0 else '')+mm.group(0),target=target)\n",
    "                retRefs+=1\n",
    "                tail=tail[mm.span()[1]:]\n",
    "                if dbg: print(f'     {head=} {e=} {tail=}')\n",
    "                _splitSpan(span,head,[e],tail)\n",
    "                chPos0=0 # span changed\n",
    "            # print(span)\n",
    "    return retRefs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# references to chapter only\n",
    "def _mkXrefChap(top):\n",
    "    patChap=re.compile(r'''\n",
    "        ((Ch.|Chapter)\\s+(?P<num>([IVX]{1,5})))\n",
    "    ''',re.X)\n",
    "    ret=0\n",
    "    for span in top.findall('.//span'):\n",
    "        if span.text is None: continue\n",
    "        pgno=int(span.getparent().attrib.get('page_id',\"0\"))\n",
    "        pos0=0\n",
    "        while chapm:=patChap.search(span.text,pos=pos0):\n",
    "            ret+=1\n",
    "            head,tail=_matchHeadTail(chapm)\n",
    "            pos0=chapm.span()[1]\n",
    "            chap=chapm.group('num')\n",
    "            _splitSpan(span,head,[mkVismRef(text=chapm.group(0),target=chapm.group('num'))],tail)\n",
    "    return ret\n",
    "        \n",
    "     \n",
    "## references to bibliography\n",
    "refRomDec=re.compile(r'(?P<book>S|M|D|A|A-a|Dhp-a|J-a|M-a|Paṭis|S-a|Vism|Nidd|Paṭṭh|Vin)(?P<loc>\\s+[XIVC]+\\s+[0-9–]+(f\\.)?)')\n",
    "\n",
    "# |Nidd\\s+I+|Paṭṭh\\s+I+|Vin\\s+[IV]+\n",
    "refDec=re.compile(r'''\n",
    "    # (\\b|^)\n",
    "    (?P<book>Sn|Ud|Cp|Cp-a|Dhp|Dhs|Dhs-a|Dhs-ṭ|Dhātuk|It|Kv|Kathāvatthu|Mil|Netti|Nikāya-s|Paṭis-a|Peṭ|Pv|Sn-a|Th|Vibh|Vibh-a|Vibh-ṭ|Vv|Vism-mhṭ|Vism mhṭ)\n",
    "    (?P<loc>\n",
    "        (\\s+|,)\n",
    "        (\n",
    "            [0-9§.–]+\n",
    "            |\n",
    "            \\(p.\\s*[0-9.–]+\\)\n",
    "            |\n",
    "            \\s*p\\.\\s*[0-9]+\n",
    "        )\n",
    "        (f\\.|\\b|$)?\n",
    "    )\n",
    "''',re.X)\n",
    "refMhv=re.compile(r'(?P<book>Mhv)(?P<loc>(\\s+pp\\.)?\\s+[0-9XIV.–]+)')\n",
    "# make sure the suffixed variants (like Dhs-a) come before the stem (like Dhs), otherwise the stem matches first\n",
    "refLone=re.compile(r'(^|\\b)(?P<book>Sn|Ud|Cp|Cp-a|Dhp|Dhs-a|Dhs-ṭ|Dhs|Dhātuk|Kv|Mil|Netti|Nikāya-s|Paṭis-a|Peṭ|Pv|Sn-a|Th|Vibh-a|Vibh-ṭ|Vibh|Vv|Vism-mhṭ|Vism mhṭ|A-a|Dhp|Dhp-a|J-a|M-a|Paṭis|S-a|Vin|Nidd|Paṭṭh)(\\b|$)')\n",
    "\n",
    "import unidecode\n",
    "\n",
    "bibTargetFixes={'Vism mhṭ':'Vism-mhṭ'}\n",
    "\n",
    "def _mkXrefBib(top):\n",
    "    ret=0\n",
    "    for span in top.findall('.//span'):\n",
    "        if span.text is None: continue\n",
    "        pgno=int(span.getparent().attrib.get('page_id',\"0\"))\n",
    "        while (\n",
    "               (bibm:=refRomDec.search(span.text))\n",
    "            or (bibm:=refDec.search(span.text))\n",
    "            or (bibm:=refLone.search(span.text))\n",
    "            or (bibm:=refMhv.search(span.text))\n",
    "        ):\n",
    "            ret+=1\n",
    "            head,tail=_matchHeadTail(bibm)\n",
    "            book=bibm.group('book')\n",
    "            loc=(bibm.group('loc') if 'loc' in bibm.groupdict() else None)\n",
    "            _splitSpan(span,head,[mkVismRef(text=bibm.group(0),target=bibTargetFixes.get(book,book),type='bib',loc=loc)],tail)\n",
    "    return ret\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "for obj,what in [(book,'book'),(index,'index'),(glossary,'glossary')]:\n",
    "    print(what)\n",
    "    print('  para',_mkXrefChapPar_2(obj))\n",
    "    print('  chap',_mkXrefChap(obj))\n",
    "    print('  bib ',_mkXrefBib(obj))\n",
    "   \n",
    "\n",
    "# clean linebreaks (not needed anymore) and double spaces allover the place\n",
    "\n",
    "for s in book.findall('.//span'):\n",
    "    if s.text is not None:\n",
    "        s.text=s.text.replace('\\n',' ')\n",
    "        s.text=s.text.replace('  ',' ').replace('  ',' ').replace('  ',' ')\n",
    "        # em and en dashes\n",
    "        s.text=s.text.replace('— ','—').replace('– ','–')\n",
    "        \n",
    "# rename all divs to p\n",
    "for div in book.findall('.//div'):\n",
    "    div.tag='p'\n",
    "\n",
    "open('xml/book.5b.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))\n",
    "open('xml/index.2a.xml','w').write(etree.tostring(index,encoding='unicode',pretty_print=True))\n",
    "open('xml/gloss.2a.xml','w').write(etree.tostring(glossary,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "d4ea7bfc-f686-49f3-a315-a87ffd254759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "struct-2-chapter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105620"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book=tree=etree.parse('xml/book.5b.xml',etree.XMLParser(remove_blank_text=True)).getroot()\n",
    "index=tree=etree.parse('xml/index.2a.xml',etree.XMLParser()).getroot() \n",
    "glossary=tree=etree.parse('xml/gloss.2a.xml',etree.XMLParser()).getroot() \n",
    "\n",
    "## patterns to REMOVE the hyphne, in book\n",
    "pats='''conscious- ness\n",
    "appre- hending\n",
    "Anurādha- pura\n",
    "existing- ness\n",
    "Concen- tration\n",
    "Dhamm- asaṅgaṇī\n",
    "sammappa- dhāna\n",
    "compre- hending\n",
    "aris- ing\n",
    "conscious- ness-originated\n",
    "pain- ful\n",
    "im- permanent\n",
    "disappear- ance\n",
    "understand- ing\n",
    "unaban- doned\n",
    "or-what- ever-states\n",
    "cakkhuviññāṇa- dhātuyā\n",
    "behav- iour\n",
    "differ- ence\n",
    "imperma- nence\n",
    "under- standing'''\n",
    "for s in book.findall('.//span'):\n",
    "    if s.text is None: continue\n",
    "    s.text=s.text.replace('  ',' ')\n",
    "    if s.text.endswith('- ') and (pg:=s.getnext()) is not None and pg.tag=='printed_page' and (s2:=pg.getnext()) is not None and s2.tag=='span' and s2.text is not None:\n",
    "        tail,head=s.text.split()[-1],s2.text.split()[0]\n",
    "        s.text=s.text[:-1]+head\n",
    "        s2.text=s2.text[len(head):]\n",
    "    for p in pats.split('\\n'):\n",
    "        if p in s.text: s.text=s.text.replace(p,p.replace('- ',''))\n",
    "    s.text=s.text.replace('- ','-')\n",
    "\n",
    "## patterns to KEEP the hyphen, in index and glossary\n",
    "pats='''power- wielder\n",
    "adhered- to\n",
    "not- self\n",
    "adukkham- asukha\n",
    "### questionmarks make the pattern not match, check how it should be\n",
    "?-suta- muta\n",
    "?kāma- bhava\n",
    "?kalyāṇa- puthujjana\n",
    "?sampatta- visaya'''\n",
    "for TOP in index,glossary:\n",
    "    for s in TOP.findall('.//span'):\n",
    "        if s.text is None: continue\n",
    "        s.text=s.text.replace('  ',' ')\n",
    "        for p in pats.split('\\n'):\n",
    "            if p in s.text: s.text.replace('  ',' ').replace(p,p.replace('- ','-'))\n",
    "        s.text=s.text.replace('  ',' ').replace('- ','')\n",
    "\n",
    "for TOP in index,glossary,book:\n",
    "    for s in TOP.findall('.//span[@family=\"italic\"]'):\n",
    "        if len(s.attrib)>1: continue\n",
    "        assert len(s.attrib)==1\n",
    "        del s.attrib['family']\n",
    "        s.tag='em'\n",
    "# move namo tassa after chapter title\n",
    "for namotassa in book.xpath('.//p[@page_id=\"63\" and @x=\"160\" and @y=126]'):\n",
    "    assert 'Namo tassa bhagavato' in namotassa[-1].text\n",
    "    #print('@')\n",
    "    # namotassa.getparent().remove(namotassa)\n",
    "    n=namotassa.getnext()\n",
    "    print(n.tag)\n",
    "    #print(n[0].tag)\n",
    "    assert n.tag=='struct-2-chapter' and n[0].tag=='heading-2-chapter'\n",
    "    n.insert(1,namotassa)\n",
    "    # assert \n",
    "        \n",
    "#for TOP in index,glossary,book:\n",
    "#    for s in TOP.xpath('.//span'):\n",
    "#        if s.text is None: s.getparent().remove(s)\n",
    "\n",
    "#for e in book.xpath('.//span[@style=\"border:black 1px solid\"]'):\n",
    "#    #assert len(e)==0\n",
    "#    for c in e:\n",
    "#    e.getparent().remove(e)\n",
    "       \n",
    "open('xml/book.6.xml','w').write(etree.tostring(book,encoding='unicode',pretty_print=True))\n",
    "open('xml/index.3.xml','w').write(etree.tostring(index,encoding='unicode',pretty_print=True))\n",
    "open('xml/gloss.3.xml','w').write(etree.tostring(glossary,encoding='unicode',pretty_print=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "09ceb0f8-88cb-47f6-9e31-a045d6638832",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf book.6.xml xml/book.final.xml\n",
    "!ln -sf index.3.xml xml/index.final.xml\n",
    "!ln -sf gloss.3.xml xml/gloss.final.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "21a2e7df-ac68-4940-a2d7-caad2e09ad52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56214"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## LaTeX output\n",
    "##\n",
    "book=tree=etree.parse('xml/book.final.xml',etree.XMLParser()).getroot() \n",
    "index=tree=etree.parse('xml/index.final.xml',etree.XMLParser()).getroot() \n",
    "gloss=tree=etree.parse('xml/gloss.final.xml',etree.XMLParser()).getroot() \n",
    "\n",
    "def _latex_writer(e,lev=0,ord=-1):\n",
    "    def _rep(t): return t.replace('&','\\\\&')\n",
    "    def _recurse(e,lev=lev):\n",
    "        if e.text is not None and len(e)==0: return _rep(e.text)\n",
    "        return ''.join([_latex_writer(e2,lev=lev+1,ord=ord) for ord,e2 in enumerate(e)])\n",
    "    def _nobraces(t):\n",
    "        #if t.strip().endswith(']'):\n",
    "        return t.replace('[','').replace(']','')\n",
    "        #return t\n",
    "    def _title(sect,e):\n",
    "        t=_recurse(e)\n",
    "        return f'\\\\{sect}[{_nobraces(t)}]'+'{'+t+'}\\n'\n",
    "    ret=''\n",
    "    ind=2*lev*'  '\n",
    "    if e.tag=='em':\n",
    "        assert len(e)==0\n",
    "        if e.text is None: return ''\n",
    "        return '\\\\emph{'+_rep(e.text)+'}'\n",
    "    elif e.tag=='span':\n",
    "        assert len(e)==0\n",
    "        if e.text is None: return ''\n",
    "        tx=_rep(e.text)\n",
    "        if (fam:=e.attrib.get('family',None)) is None: return tx\n",
    "        elif fam=='italic': return '\\\\emph{'+tx+'}'\n",
    "        elif fam=='bold': return '\\\\textbf{'+tx+'}'\n",
    "        elif fam=='smallcaps': return '\\\\textsc{'+tx+'}'\n",
    "        elif fam=='bold-italic': return '\\\\textbf{\\\\emph{'+tx+'}}'\n",
    "        else: raise RuntimeError(f'Unrecognized family {fam}')\n",
    "    elif e.tag=='p': return ('\\n\\n'+ind if ord>0 else '')+_recurse(e)\n",
    "    elif e.tag=='vism-para': return '\\\\par\\\\noindent\\\\textbf{§'+e.text+'.}\\\\vismHypertarget{'+e.attrib['anchor']+'}{}\\\\marginnote{\\\\footnotesize\\\\textcolor{purple}{'+e.attrib['anchor']+'}}{}\\n'+ind\n",
    "    elif e.tag=='footnote':\n",
    "        check=r'\\vismAssertFootnoteCounter{'+e.attrib['mark']+'}'\n",
    "        if 'reference_existing_footnote' in e.attrib: return check+r'\\footnotemark[\\value{footnote}]'\n",
    "        elif anchor:=e.attrib.get('anchor',None): return '\\\\footnote{'+check+'\\\\vismHypertarget{'+anchor+'}{}\\\\marginnote{\\\\footnotesize\\\\textcolor{purple}{'+anchor+'}}'+_recurse(e)+'}'\n",
    "        else: return '\\\\footnote{'+check+_recurse(e)+'}'\n",
    "    elif e.tag=='verse':\n",
    "        assert e[-1].tag=='line'\n",
    "        e[-1].attrib['last-line']=\"1\"\n",
    "        return '\\n'+ind+'\\\\begin{verse}\\n'+_recurse(e)+ind+'\\\\end{verse}\\n'\n",
    "    elif e.tag=='line': return ind+_recurse(e)+(r'\\\\{}' if not 'last-line' in e.attrib else '')+'\\n'\n",
    "    elif e.tag=='heading-1-part':\n",
    "        if e.attrib['toc_name']=='(Front)': return '' # ind+'\\\\frontmatter\\n\\n'\n",
    "        return '\\n'+ind+_title('part',e) # '\\\\part{'+_recurse(e)+'}\\n'\n",
    "    elif e.tag=='heading-2-chapter':\n",
    "        # \\label is jsut for PlasTeX which will then name the output file accordingly\n",
    "        if toc_num:=e.attrib.get('toc_num',None): return '\\n'+ind+_title('chapter',e)+ind+'\\\\vismHypertarget{'+toc_num+'}\\n'\n",
    "        else: return '\\n'+ind+_title('chapter',e)\n",
    "    elif e.tag=='heading-3-section': return '\\n'+ind+_title('section',e)\n",
    "    elif e.tag=='heading-4-subsection': return '\\n'+ind+_title('subsection',e)\n",
    "    elif e.tag in ('struct-2-chapter','struct-3-section','struct-4-subsection'): return _recurse(e)\n",
    "    elif e.tag=='struct-1-part':\n",
    "        # if e.attrib['name']=='(Front)': pre,post='' # ind+'\\\\frontmatter',''\n",
    "        if e.attrib['name']=='Part I': pre,post=ind+'\\\\mainmatter',''\n",
    "        elif e.attrib['name']=='Part III': pre,post='',ind+'\\\\appendix'\n",
    "        else: pre,post='',''\n",
    "        return pre+_recurse(e)+post\n",
    "    #elif e.tag=='footref': return '\\\\textbf{ERROR: footnote reference '+e.text+'}'\n",
    "    #elif e.tag=='footnote_separator': return r'\\textbf{ERROR: footnote\\textunderscore{}separator}'\n",
    "    elif e.tag=='printed_page':\n",
    "        #if e.attrib['edition']=='BPS2011': return r'{\\small\\textbf{\\href[page='+e.attrib['page_id']+']{PathofPurification2011.pdf}{\\{'+e.text+' ('+e.attrib['page_id']+')\\}}}}' # marginpar{['+e.text+r']}'\n",
    "        if e.attrib['edition']=='BPS2011':\n",
    "            # return r'\\marginnote[\\footnotesize\\{'+e.text+'('+e.attrib['page_id']+r')\\}]{}[-1ex]' # this is too complicated for PlasTeX\n",
    "            return r'\\marginnote{\\textcolor{teal}{\\footnotesize\\{'+e.text+'('+e.attrib['page_id']+r')\\}}}{} '\n",
    "        elif e.attrib['edition']=='PTS': return r'\\textcolor{brown}{\\textit{['+e.text+']}} '\n",
    "        assert False\n",
    "    elif e.tag=='ref':\n",
    "        if e.attrib['type']=='vism': return r'\\hyperlink{'+e.attrib['target']+r'}{'+e.text+'}{}'\n",
    "        elif e.attrib['type']=='bib': return r'\\textbf{\\cite{'+e.attrib['target']+'}'+(e.attrib['loc'] if 'loc' in e.attrib else '')+'}'\n",
    "        # r'\\fbox{'+e.text+'→'+e.attrib['target']+'}'\n",
    "        assert False\n",
    "    elif e.tag=='list':\n",
    "        #print(e.tag,e.attrib)\n",
    "        #title=e.attrib['title']\n",
    "        return r'\\chapter{'+e.attrib['title']+'}'+r'\\begin{multicols}{2}\\parskip=.2\\baselineskip\\RaggedRight\\parindent=-1em\\leftskip=1em '+_recurse(e)+r'\\end{multicols}'\n",
    "    elif e.tag=='entry':\n",
    "        return r'\\par\\textbf{'+e.attrib['title']+'} '+_recurse(e)+'\\n'\n",
    "    elif e.tag=='TODO':\n",
    "        return r'\\textbf{TODO '+e.attrib['id']+': '+e.attrib['desc']+'}'\n",
    "    raise RuntimeError(f'Unhandled tag <{e.tag}>')\n",
    "    \n",
    "open('latex/vism-body.tex','w').write(''.join(_latex_writer(e) for e in book))\n",
    "open('latex/vism-index.tex','w').write(''.join(_latex_writer(index)))\n",
    "open('latex/vism-glossary.tex','w').write(''.join(_latex_writer(gloss)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c955765-f78c-4660-a549-4967e2ff0b65",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "* maybe detect some more text headings? (subsections)\n",
    "* ? find all word from glossary and turn those to hyperlinks\n",
    "* (needed?) tag indented paragraphs, unindented paragraphs (§)\n",
    "* use dictionary to distinguish `<em>` and `<pali>` (semantically)\n",
    "\n",
    "# DONE\n",
    "* paragraphs in footnotes disappear (should be there)* paragraphs in footnotes disappear (should be there)\n",
    "* scan for bibliography references, turn them into `<ref type=\"bib\" target=\"...\">...</ref>`\n",
    "* detect chapter-only hyperlinks (Ch. XXX, Chapter XXX and such)\n",
    "* \"IV passim\" reference as chapters* \"IV passim\" reference as chapters\n",
    "* figure out point sizes and styles for remaining headings* (mostly done) figure out point sizes and styles for remaining headings\n",
    "* turn index & glossary into keyword-definition pairs (split where appropriate, glossary uses em-dash)* turn index & glossary into keyword-definition pairs (split where appropriate, glossary uses em-dash)\n",
    "* (re.DOTALL for multi-line regexps!): FIXME: some headings are eatn in XIX and XX (possibly others), perhaps some nodes get killd accidentally when headings are created?\n",
    "* sort elements in two-column parts (index) so that second column comes after the first one\n",
    "* detect all Vism. references (es. in the glossary/index) in text and turn them to hyperlinks\n",
    "* main text: em-dash+space (was em-dash + linebreak): remove space\n",
    "* undo hyphenation\n",
    "   \n",
    "   \n",
    "Hand Work\n",
    "==========\n",
    "\n",
    "* glossary: theOrder,theCommunity (fused)\n",
    "* II.2 II.10 II.24 II.34 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ac448a81-7cd8-4e25-9c06-2ce636009dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ sphinx/source/part-1.rst\n",
      "   → sphinx/source/ch-01.rst\n",
      "   → sphinx/source/ch-02.rst\n",
      "   → sphinx/source/ch-03.rst\n",
      "   → sphinx/source/ch-04.rst\n",
      "   → sphinx/source/ch-05.rst\n",
      "→ sphinx/source/part-2.rst\n",
      "   → sphinx/source/ch-06.rst\n",
      "   → sphinx/source/ch-07.rst\n",
      "→ sphinx/source/part-3.rst\n",
      "   → sphinx/source/ch-08.rst\n",
      "   → sphinx/source/ch-09.rst\n",
      "   → sphinx/source/ch-10.rst\n",
      "   → sphinx/source/ch-11.rst\n",
      "   → sphinx/source/ch-12.rst\n",
      "   → sphinx/source/ch-13.rst\n",
      "   → sphinx/source/ch-14.rst\n",
      "   → sphinx/source/ch-15.rst\n",
      "   → sphinx/source/ch-16.rst\n",
      "   → sphinx/source/ch-17.rst\n",
      "   → sphinx/source/ch-18.rst\n",
      "→ sphinx/source/part-4.rst\n",
      "   → sphinx/source/ch-19.rst\n",
      "   → sphinx/source/ch-20.rst\n",
      "   → sphinx/source/ch-21.rst\n",
      "   → sphinx/source/ch-22.rst\n",
      "   → sphinx/source/ch-23.rst\n",
      "   → sphinx/source/ch-24.rst\n",
      "   → sphinx/source/ch-25.rst\n",
      "   → sphinx/source/ch-26.rst\n",
      "   → sphinx/source/ch-27.rst\n",
      "   → sphinx/source/ch-28.rst\n",
      "→ sphinx/source/index_.rst\n",
      "→ sphinx/source/glossary.rst\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "##\n",
    "## Sphinx\n",
    "##\n",
    "book=tree=etree.parse('xml/book.final.xml',etree.XMLParser()).getroot() \n",
    "index=tree=etree.parse('xml/index.final.xml',etree.XMLParser()).getroot() \n",
    "gloss=tree=etree.parse('xml/gloss.final.xml',etree.XMLParser()).getroot() \n",
    "\n",
    "\n",
    "class SphinxWriter(object):\n",
    "    def __init__(self,outdir):\n",
    "        self.footnotes={}\n",
    "        self.outdir=outdir\n",
    "        self.chapter=0\n",
    "        self.part=0\n",
    "    def fixanchor(self,a):\n",
    "        # return a.replace('.','')\n",
    "        return a\n",
    "    def _flush(self):\n",
    "        if not self.footnotes: return ''\n",
    "        ret='\\n\\n.. rubric:: Footnotes\\n\\n'\n",
    "        # TODO: multi-paragraph footnotes\n",
    "        for k,vv in self.footnotes.items(): ret+=f'\\n\\n.. [#{k}] '+'\\n    '.join([v for v in vv.split('\\n')])+'\\n'\n",
    "        self.footnotes={}\n",
    "        return ret\n",
    "    def _rep(self,t): return t # .replace('&','\\\\&') \n",
    "    def recurse(self,e):\n",
    "        if e.text is not None and len(e)==0: return self._rep(e.text)\n",
    "        return ''.join([self.write(e2,ord=ord) for ord,e2 in enumerate(e)])\n",
    "    def title(self,e,level,anchor=None,prefix=None):\n",
    "        ret=''\n",
    "        if anchor: ret+='\\n\\n.. _'+anchor+':'\n",
    "        t=(e if isinstance(e,str) else self.recurse(e))\n",
    "        if prefix: t=prefix+'. '+t\n",
    "        return ret+'\\n\\n'+t+'\\n'+len(t)*('#*=-^\"'[level])\n",
    "    def enclose(self,t,c):\n",
    "        if t.strip()=='': return ' '\n",
    "        ret=t\n",
    "        if ret.endswith(' '): ret=ret.rstrip()+c+' '\n",
    "        else: ret=ret+c+'\\\\ '\n",
    "        if ret.startswith(' '): ret=' '+c+ret.lstrip()\n",
    "        else: ret=c+ret\n",
    "        return ret\n",
    "\n",
    "    def write(self,e,ord=-1,list_type=None):\n",
    "        if list_type is not None: self.list_type=list_type\n",
    "        def _nobraces(t):\n",
    "            #if t.strip().endswith(']'):\n",
    "            return t.replace('[','').replace(']','')\n",
    "            #return t\n",
    "        if e.tag=='em':\n",
    "            assert len(e)==0\n",
    "            if e.text is None: return ''\n",
    "            return self.enclose(self._rep(e.text),'*')\n",
    "        elif e.tag=='span':\n",
    "            assert len(e)==0\n",
    "            if e.text is None: return ''\n",
    "            tx=self._rep(e.text)\n",
    "            if (fam:=e.attrib.get('family',None)) is None: return tx\n",
    "            elif fam=='italic': return self.enclose(tx,'*')\n",
    "            elif fam=='bold': return self.enclose(tx,'**')\n",
    "            elif fam=='smallcaps': return self.enclose(tx,'``')\n",
    "            elif fam=='bold-italic': return self.enclose(tx,'``')\n",
    "            else: raise RuntimeError(f'Unrecognized family {fam}')\n",
    "        elif e.tag=='p': return ('\\n\\n' if ord>0 else '')+self.recurse(e)\n",
    "        elif e.tag=='vism-para': return f'\\n\\n.. _{self.fixanchor(e.attrib[\"anchor\"])}:\\n\\n**§{e.text}** '\n",
    "        elif e.tag=='footnote':\n",
    "            anchor=self.fixanchor(e.attrib.get(\"anchor\",str(len(self.footnotes)+1)))\n",
    "            if 'reference_existing_footnote' in e.attrib: return '[#{anchor}]_'\n",
    "            self.footnotes[anchor]=self.recurse(e)\n",
    "            return f' [#{anchor}]_ '\n",
    "        elif e.tag=='verse':\n",
    "            return '\\n\\n'+self.recurse(e)\n",
    "        elif e.tag=='line': return ('\\n\\n' if ord==0 else '')+'\\n| '+self.recurse(e)+('\\n' if 'last-line' in e.attrib else '')\n",
    "        elif e.tag=='heading-1-part':\n",
    "            self.partOut.write(self.title(e,level=1,prefix=e.attrib[\"toc_name\"])+'\\n\\n.. toctree::\\n   :numbered:\\n   :maxdepth: 6\\n\\n')\n",
    "            return ''\n",
    "        elif e.tag=='heading-2-chapter':\n",
    "            return self.title(e,level=2,anchor=e.attrib.get('toc_num',None),prefix=e.attrib.get('toc_num',None))\n",
    "        elif e.tag=='heading-3-section':\n",
    "            return self.title(e,level=3)\n",
    "        elif e.tag=='heading-4-subsection':\n",
    "            return self.title(e,level=4)\n",
    "        elif e.tag in ('struct-3-section','struct-4-subsection'): return self.recurse(e)\n",
    "        elif e.tag=='struct-1-part':\n",
    "            self.part+=1\n",
    "            f=f'{self.outdir}/part-{self.part}.rst'\n",
    "            self.partOut=open(f,'w')\n",
    "            print(f'→ {f}')\n",
    "            for chap in e:\n",
    "                self.partOut.write('\\n   '+self.write(chap))\n",
    "            self.partOut.close()\n",
    "            return None\n",
    "        elif e.tag=='struct-2-chapter':\n",
    "            self.chapter+=1\n",
    "            f=f'ch-{self.chapter:02d}.rst'\n",
    "            ff=f'{self.outdir}/{f}'\n",
    "            print(f'   → {ff}')\n",
    "            out=open(ff,'w')\n",
    "            out.write(self.recurse(e)+self._flush())\n",
    "            return f\n",
    "        elif e.tag=='book':\n",
    "            for e2 in e: self.write(e2)\n",
    "            return None\n",
    "        elif e.tag=='footref': return f'[#{e.text}]_'\n",
    "        elif e.tag=='printed_page':\n",
    "            #if e.attrib['edition']=='BPS2011': return r'{\\small\\textbf{\\href[page='+e.attrib['page_id']+']{PathofPurification2011.pdf}{\\{'+e.text+' ('+e.attrib['page_id']+')\\}}}}' # marginpar{['+e.text+r']}'\n",
    "            if e.attrib['edition']=='BPS2011': return f'*[{e.text}/{e.attrib[\"page_id\"]}]*\\n'\n",
    "            elif e.attrib['edition']=='PTS': return f' ``{e.text}`` '\n",
    "            assert False\n",
    "        elif e.tag=='ref':\n",
    "            if e.attrib['type']=='vism':\n",
    "                #return f'`{e.text} <{self.fixanchor(e.attrib[\"target\"])}>`_ '\n",
    "                return f':ref:`{e.text} <{self.fixanchor(e.attrib[\"target\"])}>` '\n",
    "            elif e.attrib['type']=='bib': return f' [{e.attrib[\"target\"]}]_ '+(self.enclose(e.attrib[\"loc\"],'*') if 'loc' in e.attrib else '')+' '\n",
    "            assert False\n",
    "        elif e.tag=='list':    \n",
    "            if self.list_type=='index': name,ret='index_',self.title(e='Index',level=1,anchor='index')+'\\n\\n.. glossary::'\n",
    "            if self.list_type=='glossary': name,ret='glossary',self.title(e='Glossary',level=1,anchor='glossary')+'\\n\\n.. glossary::'\n",
    "            ret+=self.recurse(e)\n",
    "            f=f'{self.outdir}/{name}.rst'\n",
    "            print(f'→ {f}')\n",
    "            open(f,'w').write(ret)\n",
    "            return\n",
    "            # return r'\\chapter{'+e.attrib['title']+'}'+r'\\begin{multicols}{2}\\parskip=.2\\baselineskip\\RaggedRight\\parindent=-1em\\leftskip=1em '+_recurse(e)+r'\\end{multicols}'\n",
    "        elif e.tag=='entry':\n",
    "            #if self.list_type=='index':\n",
    "            #    return f'\\n   {e.attrib[\"title\"]}: '+self.recurse(e)\n",
    "            #elif self.list_type=='glossary':\n",
    "            title=e.attrib[\"title\"].replace(\"*\",\"\\\\*\")\n",
    "            return f'\\n\\n   {title}\\n          '+self.recurse(e) # {e.attrib[\"desc\"]}'\n",
    "        elif e.tag=='TODO':\n",
    "            return f'\\n\\n.. todo:: {e.attrib[\"id\"]}\\n\\n    e.attrib[\"desc\"]'\n",
    "        raise RuntimeError(f'Unhandled tag <{e.tag}>')\n",
    "    \n",
    "writer=SphinxWriter(outdir='sphinx/source')\n",
    "writer.write(book)\n",
    "writer.write(index,list_type='index')\n",
    "writer.write(gloss,list_type='glossary')\n",
    "# ''.join(_latex_writer(e) for e in book))\n",
    "#open('/tmp/vism-index.tex','w').write(''.join(_latex_writer(index,list='index')))\n",
    "#open('/tmp/vism-glossary.tex','w').write(''.join(_latex_writer(gloss,list='glossary')))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a923735-e054-4dbd-9d02-3c471ca57d66",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
